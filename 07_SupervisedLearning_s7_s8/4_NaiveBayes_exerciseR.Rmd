---
title: "Practical Implementation of Naive Bayes In R"
output: html_notebook
---

Exercise of https://www.edureka.co/blog/naive-bayes-in-r/

Problem Statement: To study a Diabetes data set and build a Machine Learning model that predicts whether or not a person has Diabetes.

Data Set Description: The given data set contains 100s of observations of patients along with their health details. Here’s a list of the predictor variables that will help us classify a patient as either Diabetic or Normal:

Pregnancies: Number of pregnancies so far
Glucose: Plasma glucose concentration
BloodPressure: Diastolic blood pressure (mm Hg)
SkinThickness: Triceps skin fold thickness (mm)
Insulin: 2-Hour serum insulin (mu U/ml)
BMI: Body mass index (weight in kg/(height in m)^2)
DiabetesPedigreeFunction: Diabetes pedigree function
Age: Age (years)
The response variable or the output variable is:

Outcome: Class variable (0 or 1)
Logic: To build a Naive Bayes model in order to classify patients as either Diabetic or normal by studying their medical records such as Glucose level, age, BMI, etc.

```{r}
#Loading required packages
install.packages('tidyverse')
library(tidyverse)
install.packages('ggplot2')
library(ggplot2)
install.packages('caret')
library(caret)
install.packages('caretEnsemble')
library(caretEnsemble)
install.packages('psych')
library(psych)
install.packages('Amelia')
library(Amelia)
install.packages('mice')
library(mice)
install.packages('GGally')
library(GGally)
install.packages('rpart')
library(rpart)
install.packages('randomForest')
library(randomForest)
```

```{r}
#Reading data into R
data<- read.csv("datasetsR/diabetes.csv")
```

```{r}
#Setting outcome variables as categorical
data$Outcome <- factor(data$Outcome, levels = c(0,1), labels = c("False", "True"))
```

```{r}
#Studying the structure of the data
str(data)
```

```{r}
#Understanding the data set
head(data)
describe(data)
```

Data Cleaning

While analyzing the structure of the data set, we can see that the minimum values for Glucose, Bloodpressure, Skinthickness, Insulin, and BMI are all zero. This is not ideal since no one can have a value of zero for Glucose, blood pressure, etc. Therefore,  such values are treated as missing observations.

In the below code snippet, we’re setting the zero values to NA’s:

```{r}
# Convert '0' values into NA
data[, 2:7][data[, 2:7] == 0] <- NA

#visualize the missing data
missmap(data)
```

The above illustrations show that our data set has plenty missing values and removing all of them will leave us with an even smaller data set, therefore, we can perform imputations by using the mice package in R.

```{r}
#Use mice package to predict missing values
mice_mod <- mice(data[, c("Glucose","BloodPressure","SkinThickness","Insulin","BMI")], method='rf')
mice_complete <- complete(mice_mod)

#Transfer the predicted missing values into the main data set
data$Glucose <- mice_complete$Glucose
data$BloodPressure <- mice_complete$BloodPressure
data$SkinThickness <- mice_complete$SkinThickness
data$Insulin<- mice_complete$Insulin
data$BMI <- mice_complete$BMI
```

To check if there are still any missing values, let’s use the missmap plot:

```{r}
missmap(data)
```

The output looks good, there is no missing data.

Exploratory Data Analysis

Now let’s perform a couple of visualizations to take a better look at each variable, this stage is essential to understand the significance of each predictor variable.

```{r}
#Data Visualization
#Visual 1
ggplot(data, aes(Age, colour = Outcome)) +
geom_freqpoly(binwidth = 1) + labs(title="Age Distribution by Outcome")

```


```{r}
#visual 2
c <- ggplot(data, aes(x=Pregnancies, fill=Outcome, color=Outcome)) +
geom_histogram(binwidth = 1) + labs(title="Pregnancy Distribution by Outcome")
c + theme_bw()
```


```{r}
#visual 3
P <- ggplot(data, aes(x=BMI, fill=Outcome, color=Outcome)) +
geom_histogram(binwidth = 1) + labs(title="BMI Distribution by Outcome")
P + theme_bw()
```

```{r}
#visual 4
ggplot(data, aes(Glucose, colour = Outcome)) +
geom_freqpoly(binwidth = 1) + labs(title="Glucose Distribution by Outcome")

```

```{r}
#visual 5
ggpairs(data)
```

Data Modelling

This stage begins with a process called Data Splicing, wherein the data set is split into two parts:

Training set: This part of the data set is used to build and train the Machine Learning model.
Testing set: This part of the data set is used to evaluate the efficiency of the model.

```{r}
#Building a model
#split data into training and test data sets
indxTrain <- createDataPartition(y = data$Outcome,p = 0.75,list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
 
#Check dimensions of the split
prop.table(table(data$Outcome)) * 100
prop.table(table(training$Outcome)) * 100
prop.table(table(testing$Outcome)) * 100
 
```
For comparing the outcome of the training and testing phase let’s create separate variables that store the value of the response variable:

```{r}
#create objects x which holds the predictor variables and y which holds the response variables
x = training[,-9]
y = training$Outcome
```

Now it’s time to load the e1071 package that holds the Naive Bayes function. This is an in-built function provided by R.

```{r}
library(e1071)
```

After loading the package, the below code snippet will create Naive Bayes model by using the training data set:

```{r}
model = train(x,y,'nb',trControl=trainControl(method='cv',number=10))
model
```

We thus created a predictive model by using the Naive Bayes Classifier.

Step 7: Model Evaluation

To check the efficiency of the model, we are now going to run the testing data set on the model, after which we will evaluate the accuracy of the model by using a Confusion matrix.

```{r}
#Model Evaluation
#Predict testing set
Predict <- predict(model,newdata = testing )

#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(Predict, testing$Outcome )
```

The final output shows that we built a Naive Bayes classifier that can predict whether a person is diabetic or not, with an accuracy of approximately 73%.

To summaries the demo, let’s draw a plot that shows how each predictor variable is independently responsible for predicting the outcome.

```{r}
#Plot Variable performance
X <- varImp(model)
plot(X)
```

From the above illustration, it is clear that ‘Glucose’ is the most significant variable for predicting the outcome.
