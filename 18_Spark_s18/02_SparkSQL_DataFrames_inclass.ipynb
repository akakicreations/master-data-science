{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1W1D1mPbDGvy"
   },
   "source": [
    "# SparkSQL and DataFrames \n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tema de collab: \n",
    "    \n",
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "ROOT = '/content/drive'     # default for the drive\n",
    "PROJ = 'Drive access Data Science'       # path to your project on Drive\n",
    "\n",
    "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
    "\n",
    "PROJECT_PATH = join(ROOT, PROJ)\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdESfq7rDGv3"
   },
   "source": [
    "## RDDs, DataSets, and DataFrames\n",
    "\n",
    "RDDs are the original interface for Spark programming.\n",
    "\n",
    "DataFrames were introduced in 1.3\n",
    "\n",
    "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9bk6mqqDGv6"
   },
   "source": [
    "### Advantages of DataFrames:\n",
    "\n",
    "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
    "\n",
    "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2k1UFbFDGv_"
   },
   "source": [
    "## SparkSQL and DataFrames \n",
    "\n",
    "\n",
    "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
    "\n",
    "From https://spark.apache.org/docs/2.4.4/sql-programming-guide.html\n",
    "\n",
    "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgoFtnisDGwE"
   },
   "source": [
    "### The pyspark.sql module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENLGO-dFDGwH"
   },
   "source": [
    "Important classes of Spark SQL and DataFrames:\n",
    "\n",
    "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
    "\n",
    "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
    "\n",
    "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
    "\n",
    "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
    "\n",
    "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
    "\n",
    "* `pyspark.sql.types` List of data types available.\n",
    "\n",
    "* `pyspark.sql.Window` For working with window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTx0dOg0DGwK"
   },
   "source": [
    "http://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocdELJEWDGwQ"
   },
   "source": [
    "## SparkSession\n",
    "\n",
    "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
    "\n",
    "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5To3q5kDGwW"
   },
   "source": [
    "### If you are running this notebook in Google Colab\n",
    "\n",
    "Copy the following to a code cell and run it. It will install and set up Spark for you.\n",
    "\n",
    "```python\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark pyspark==2.4.6\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "fp6qy7qVDGwZ",
    "outputId": "668566e0-18d1-4da3-bb09-6deabdc79133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 218.4MB 66kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 49.4MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark pyspark==2.4.6\n",
    " \n",
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ks5c3ncDGwu"
   },
   "source": [
    "#### Passing other options to spark session:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTcj4eFoDGww"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('someconfigkey', 'someconfigvalue').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pH8oAEzHDGw_"
   },
   "source": [
    "We can check option values in the resulting session like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "hrFX2JY0DGxC",
    "outputId": "76b307ff-9435-4657-ee26-f23e81611636"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1595056035824'),\n",
       " ('spark.driver.port', '35545'),\n",
       " ('spark.driver.host', '07272e473bf8'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('someconfigkey', 'someconfigvalue')]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0wsa4JfDGxX"
   },
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "B-7764utDGxZ",
    "outputId": "534559ba-6a48-4a27-9ae4-66aac4f475a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'orc'),\n",
       " (1, 'elf'),\n",
       " (2, 'elf'),\n",
       " (3, 'elf'),\n",
       " (4, 'halfling'),\n",
       " (5, 'halfling'),\n",
       " (6, 'halfling'),\n",
       " (7, 'elf'),\n",
       " (8, 'orc'),\n",
       " (9, 'elf'),\n",
       " (10, 'elf'),\n",
       " (11, 'orc'),\n",
       " (12, 'elf'),\n",
       " (13, 'elf'),\n",
       " (14, 'orc'),\n",
       " (15, 'orc'),\n",
       " (16, 'elf'),\n",
       " (17, 'orc'),\n",
       " (18, 'halfling'),\n",
       " (19, 'elf')]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "n = 20\n",
    "creatures = random.choices(['elf', 'orc', 'halfling'], k=n)\n",
    "\n",
    "rows = list(zip(range(n), creatures))\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v0mCom91GX1h",
    "outputId": "93893260-9e2a-4317-ce50-19ad5ed3caec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "oKibVa6nGkj8",
    "outputId": "a12497d6-e09b-48cd-a3d6-ff7c632a05da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| _1|      _2|\n",
      "+---+--------+\n",
      "|  0|     orc|\n",
      "|  1|     elf|\n",
      "|  2|     elf|\n",
      "|  3|     elf|\n",
      "|  4|halfling|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JBtsgIZyGvpO",
    "outputId": "483e2cb0-0ec5-4924-f26b-79c5b41266e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "A3X-53gqG0aY",
    "outputId": "2269a883-7a9e-410e-bc8e-f28611693a56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='orc'),\n",
       " Row(_1=1, _2='elf'),\n",
       " Row(_1=2, _2='elf'),\n",
       " Row(_1=3, _2='elf'),\n",
       " Row(_1=4, _2='halfling')]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gu1V44ImHIzW",
    "outputId": "572a861a-0348-43a2-e964-799af4ac149f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(x=1, y=0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "\n",
    "a_row = Row(x=1, y=0.5)\n",
    "a_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "luBPyhMCHXgh",
    "outputId": "7d3785d5-c98e-4b59-9ae1-ea56371f30ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_row['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH8p_KbXDGxq"
   },
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "* From RDDs\n",
    "* from Hive tables\n",
    "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UZK7TQRhHhQ0",
    "outputId": "ad6dd3d0-1e69-4002-eafd-f9f3f1ed81ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, race: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(rows, schema=['id', 'race'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_BtZF00DGxr"
   },
   "source": [
    "#### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "J99dtfrgDGxv",
    "outputId": "85ccf369-be2d-49c0-d034-5a611e7df7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-18 07:09:55--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5777367 (5.5M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   5.51M  1.33MB/s    in 4.1s    \n",
      "\n",
      "2020-07-18 07:09:59 (1.33 MB/s) - ‘shakespeare.txt’ saved [5777367/5777367]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -v http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "D1zIdJtSIHUq",
    "outputId": "3a774b04-fefc-4737-93b2-9ba01954129f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 107),\n",
       " ('of', 18811),\n",
       " ('shakespeare', 13),\n",
       " ('this', 7177),\n",
       " ('ebook', 14)]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(line):\n",
    "  return re.sub('[^a-z0-9 ]', '', line.lower())\n",
    "\n",
    "shakespeare = spark.sparkContext.textFile('shakespeare.txt')\n",
    "\n",
    "freqs = shakespeare.map(remove_punctuation)\\\n",
    "                   .flatMap(str.split)\\\n",
    "                   .map(lambda word: (word, 1))\\\n",
    "                   .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "freqs.take(5)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MdaIZM0vJLFk",
    "outputId": "7a649ac8-8563-478a-dbbf-e59a9845d223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, times: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_df = spark.createDataFrame(freqs, schema=['word', 'times'])\n",
    "freqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwwEMWimDGx_"
   },
   "source": [
    "### Inferring and specifying schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "A7rsfbTHDGyB",
    "outputId": "daa14891-8bab-4da6-b792-287859a6c251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEXYNzrfDGyT"
   },
   "source": [
    "#### Fully specifying a schema\n",
    "\n",
    "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WySUSjj8DGyV",
    "outputId": "bf7da70b-7456-4c1f-9663-5ee297cddd0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoubleType"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "types.IntegerType()\n",
    "types.FloatType()\n",
    "types.DoubleType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y71I3VLRKjAZ",
    "outputId": "33d0ef2f-029f-4a77-ff9e-192923941718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(x,FloatType,true),StructField(y,FloatType,true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_type = types.StructType(fields=[types.StructField('x', types.FloatType()), \n",
    "                                      types.StructField('y', types.FloatType())])\n",
    "\n",
    "coord_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUyct_-pDGyi"
   },
   "source": [
    "#### From csv files\n",
    "\n",
    "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "Xp8-7C4RLxjH",
    "outputId": "1892c471-93cd-4d46-bd4b-745eaa3c001f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\n",
      "79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0\n",
      "79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0\n",
      "79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0\n",
      "79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0\n"
     ]
    }
   ],
   "source": [
    "!zcat coupon150720.csv.gz | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zxtUGp3SDGyl",
    "outputId": "632b63b7-420f-4692-f71a-f9a13331655c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: bigint, _c1: int, _c2: string, _c3: string, _c4: string, _c5: string, _c6: double, _c7: string, _c8: int, _c9: string, _c10: string, _c11: string, _c12: int, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons = spark.read.csv('coupon150720.csv.gz', inferSchema=True)\n",
    "coupons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W-nPIx9GMaM5",
    "outputId": "12fbd2ac-f61b-4b2f-d9c6-4807290c799b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tkt_number: string, origin: string, dest: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons = spark.sql('''SELECT _c0 AS tkt_number, \n",
    "                              _c2 AS origin,  \n",
    "                              _c3 AS dest\n",
    "                       FROM csv.`coupon150720.csv.gz`''')\n",
    "coupons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a-YvdT7DGy8"
   },
   "source": [
    "#### From other types of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiBDDssgDGy_"
   },
   "source": [
    "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QIlE_rnqDGzA",
    "outputId": "056067bf-7b84-4873-fd50-5d7e6a277b36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameReader.json of <pyspark.sql.readwriter.DataFrameReader object at 0x7febe68cbf98>>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet\n",
    "spark.read.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8RYleWtDGzR"
   },
   "source": [
    "### Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "-lFsKLaRDGzV",
    "outputId": "f7c7ad1d-b551-40c4-8b01-64bdb0f964ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    race|\n",
      "+---+--------+\n",
      "|  0|     orc|\n",
      "|  1|     elf|\n",
      "|  2|     elf|\n",
      "|  3|     elf|\n",
      "|  4|halfling|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "vzM-XgtjPKp2",
    "outputId": "00231235-78e7-4426-983f-f9843794e80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmNJ7fWcDGzl"
   },
   "source": [
    "### Filtering and selecting\n",
    "\n",
    "Syntax inspired in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m_4lY-OPDGzo",
    "outputId": "adc5b3b0-8eae-46ac-e3d1-b854d50a5b83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rIxWKdOJPdVo",
    "outputId": "e8bf5353-6c3f-40a6-de5d-885b65e0a43c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tkt_number: string, origin: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons.select('tkt_number',\n",
    "               'origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5FHYtdv_Pk_N",
    "outputId": "d878c239-738d-4708-de44-7048f0fcb0bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tkt_number: string, origin: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons[['tkt_number', 'origin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUc2ltu3DGz3"
   },
   "source": [
    "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4FpeJsdKDGz5",
    "outputId": "de6d3f84-403f-43d0-e4ca-e3ef64dba4f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'id'>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ljG0HQTuQv7e",
    "outputId": "aca705ae-9abd-4613-d5bb-f536369e36d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "IsNqmVEqRKeR",
    "outputId": "ef948882-8d0b-4eab-cd80-190d67150d6e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a80c23594027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df['id'].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JGI3H9pZRVD2",
    "outputId": "d185b859-6a91-4c63-98a0-f42ece1532ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(id > 5)'>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id'] > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhULM-y5DG0L"
   },
   "source": [
    "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "JBeFv4CtDG0Q",
    "outputId": "1688a508-54c7-481c-9775-7055e7bb9eb7",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    race|\n",
      "+---+--------+\n",
      "|  0|     orc|\n",
      "|  1|     elf|\n",
      "|  2|     elf|\n",
      "|  3|     elf|\n",
      "|  4|halfling|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df['id'] < 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wbUXsyIpR2f7",
    "outputId": "8cf6fe25-26e7-4020-c470-f834682af9e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These return the same\n",
    "df.select(df['id'])\n",
    "df.select('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6HC-tL1GSNqh",
    "outputId": "eef857de-e22e-46e1-a740-0be160fb2bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'id'>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "BK14HOlXSWPD",
    "outputId": "7f1be8e0-6615-436b-c6a3-3b3bcaf32142"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4509c9174cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'id'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "'id' < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "ZQNrKmRJSD9r",
    "outputId": "fcdc4edb-56a9-4e0e-d0d3-90f8e6e8b9d1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fb69e4b2adb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "df.where('id' < 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dK9HbYirDG0c"
   },
   "source": [
    "`where` is exactly synonimous with `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "cRJ9QVT_DG0g",
    "outputId": "2b5de540-a1eb-44b5-f610-3be5ee1effe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    race|\n",
      "+---+--------+\n",
      "|  0|     orc|\n",
      "|  1|     elf|\n",
      "|  2|     elf|\n",
      "|  3|     elf|\n",
      "|  4|halfling|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['id'] < 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nE5MJnyDG0q"
   },
   "source": [
    "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZUOMnQeDG04"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Extract all employee ids which correspond to orcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "31wYocjIDG08",
    "outputId": "bec5b85e-d1e6-4463-f46e-04e4eb7b428c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  8|\n",
      "| 11|\n",
      "| 14|\n",
      "| 15|\n",
      "| 17|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df['race'] == 'orc').select('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "sKVbQfHnUBRN",
    "outputId": "6fa14df0-7078-47c8-dcd6-3afd1b1577f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  8|\n",
      "| 11|\n",
      "| 14|\n",
      "| 15|\n",
      "| 17|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df['race'] == 'orc'][['id']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8dibNBPDG1V"
   },
   "source": [
    "### Adding columns\n",
    "\n",
    "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "hNwmjKs1DG1Y",
    "outputId": "e8b7672a-e74e-4398-d606-b6d429a9b338",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-361a611c0489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "df['number'] = df['id'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIHC-hF7Uija",
    "outputId": "d0a9f747-bb14-468c-d5b8-ed45644f48cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, race: string, number: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('number', df['id'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hpQRwS9VU7oB",
    "outputId": "30ad49d4-293e-44d3-d09f-1e60d333ef5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'andoi'"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'and'\n",
    "a += 'oi'\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xtR7IlODG1m"
   },
   "source": [
    "### User defined functions\n",
    "\n",
    "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
    "\n",
    "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "w-flictADG1q",
    "outputId": "86757186-52c5-4625-ba9d-8ed3566bb02a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8b7d6f064158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog1p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not Column"
     ]
    }
   ],
   "source": [
    "from math import log1p \n",
    "\n",
    "log1p(df['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukTHoL0aDG18"
   },
   "source": [
    "This errors out because \n",
    "\n",
    "```python\n",
    "math.log1p\n",
    "```\n",
    "\n",
    "is not a udf: it doesn't know how to work with strings or Column objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "f6i-i2D4DG1-",
    "outputId": "f7de4de1-6933-4c0d-bd41-c0a9341f6318",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+\n",
      "| id|    race|         newcolumn|\n",
      "+---+--------+------------------+\n",
      "|  0|     orc|               0.0|\n",
      "|  1|     elf|0.6931471805599453|\n",
      "|  2|     elf|1.0986122886681096|\n",
      "|  3|     elf|1.3862943611198906|\n",
      "|  4|halfling|1.6094379124341003|\n",
      "+---+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "df.withColumn('newcolumn', functions.log1p(df['id'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grksVgSsDG2P"
   },
   "source": [
    "But we can transform it into a udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Jlc4q0OPDG2R",
    "outputId": "6a329933-dabe-45a5-8e15-5262d6d3d9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+\n",
      "| id|    race|newcolumn|\n",
      "+---+--------+---------+\n",
      "|  0|     orc|        C|\n",
      "|  1|     elf|        F|\n",
      "|  2|     elf|        F|\n",
      "|  3|     elf|        F|\n",
      "|  4|halfling|        G|\n",
      "+---+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_random_udf = functions.udf(lambda word: word[-1].upper())\n",
    "df.withColumn('newcolumn', my_random_udf(df['race'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_Jh2l5CDG2i"
   },
   "source": [
    "We can do the same with any function we dream up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Mmu8-CHcDG2l",
    "outputId": "d04c0490-a19b-4dd8-f1e2-42bb646abe91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    race|absur|\n",
      "+---+--------+-----+\n",
      "|  0|     orc|    9|\n",
      "|  1|     elf|   16|\n",
      "|  2|     elf|   25|\n",
      "|  3|     elf|   36|\n",
      "|  4|halfling|  144|\n",
      "+---+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def absurd_function(a, b):\n",
    "  return (a + len(b)) ** 2\n",
    "\n",
    "my_absurd_udf = functions.udf(absurd_function)\n",
    "\n",
    "df2 = df.withColumn('absur', my_absurd_udf('id', 'race'))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGeSfRHEDG20"
   },
   "source": [
    "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "g5AdMHftDG23",
    "outputId": "6a7a6d3e-379f-4577-8d48-423ddba52847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- absur: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReOOEMjtDG3F"
   },
   "source": [
    "Think about this function: what is its return type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ij6uRD7hDG3H"
   },
   "outputs": [],
   "source": [
    "def incognito(a, b):\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "zh0O6DFA4Hd6",
    "outputId": "42e138aa-56ba-4fc4-f468-59a61e96c8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- absur: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def absurd_function(a, b):\n",
    "  return (a + len(b)) ** 2\n",
    "\n",
    "my_absurd_udf = functions.udf(absurd_function, returnType=types.IntegerType())\n",
    "\n",
    "df2 = df.withColumn('absur', my_absurd_udf('id', 'race'))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmiZM-yY6Ix-"
   },
   "source": [
    "If we have a column that is not the desired type, we can convert it with `cast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "QLDhJ5IY6PFA",
    "outputId": "21db021c-cb12-4355-de79-e65a70dd60cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- absur: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('absur', my_absurd_udf('id', 'race').cast(types.FloatType()))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBYSVQQUDG3W"
   },
   "source": [
    "#### Exercise: \n",
    "\n",
    "Create a 'hitpoints' field in our df. make it 30000 for halflings, 40000 for elves and 70000 for orcs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "xCW_GLauDG3Y",
    "outputId": "b01bfe06-1a2d-4d60-9736-f13d05963a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    race|\n",
      "+---+--------+\n",
      "|  0|     orc|\n",
      "|  1|     elf|\n",
      "|  2|     elf|\n",
      "|  3|     elf|\n",
      "|  4|halfling|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "elu6edH68ps7",
    "outputId": "b859b773-98fa-45e8-f2de-3d657238eb33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hp(race):\n",
    "  hps = {'elf' : 40000, 'halfling' : 30000, 'orc' : 70000}\n",
    "  return hps[race]\n",
    "\n",
    "hp('elf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "isK3LdzW9CVf",
    "outputId": "651ebc5e-cfab-453c-899b-7e1f48c8b23d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    race|   hp|\n",
      "+---+--------+-----+\n",
      "|  0|     orc|70000|\n",
      "|  1|     elf|40000|\n",
      "|  2|     elf|40000|\n",
      "|  3|     elf|40000|\n",
      "|  4|halfling|30000|\n",
      "+---+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hp_udf = functions.udf(hp, returnType=types.IntegerType())\n",
    "\n",
    "df3 = df.withColumn('hp', hp_udf('race'))\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ph7NR5HBPuK1",
    "outputId": "1cf7c63e-549e-4ca0-8763-dba499e8cde2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ei4S8ZlJPxXX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "6uS1DeycCKDo",
    "outputId": "beb2d250-7ec2-40cf-fb2a-d3d3620f7ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----+\n",
      "| id|    race|   hp|    x|\n",
      "+---+--------+-----+-----+\n",
      "|  0|     orc|70000|70000|\n",
      "|  1|     elf|40000|40000|\n",
      "|  2|     elf|40000|40000|\n",
      "|  3|     elf|40000|40000|\n",
      "|  4|halfling|30000|30000|\n",
      "|  5|halfling|30000|30000|\n",
      "|  6|halfling|30000|30000|\n",
      "|  7|     elf|40000|40000|\n",
      "|  8|     orc|70000|70000|\n",
      "|  9|     elf|40000|40000|\n",
      "| 10|     elf|40000|40000|\n",
      "| 11|     orc|70000|70000|\n",
      "| 12|     elf|40000|40000|\n",
      "| 13|     elf|40000|40000|\n",
      "| 14|     orc|70000|70000|\n",
      "| 15|     orc|70000|70000|\n",
      "| 16|     elf|40000|40000|\n",
      "| 17|     orc|70000|70000|\n",
      "| 18|halfling|30000|30000|\n",
      "| 19|     elf|40000|40000|\n",
      "+---+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.withColumn('x', functions.when(df3['race'] == 'orc', 70000)\n",
    "                             .when(df3['race'] == 'elf', 40000)\n",
    "                             .otherwise(30000)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgQyvaVLDG4C"
   },
   "source": [
    "### Summary statistics\n",
    "\n",
    "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wcipDQ8bDG4G",
    "outputId": "56353965-afb2-40db-b426-4d808cb09c7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16198084347928346"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.stat.corr('id', 'hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1Ku8AlYO96Cm",
    "outputId": "319362c1-bb2b-4d63-80ed-bb17bd58c4e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15263.157894736842"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.stat.cov('id', 'hp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_ug_AfODG4W"
   },
   "source": [
    "### .crosstab()\n",
    "\n",
    "Crosstab returns the contingency table for two columns, as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "nen9cfu0DG4Z",
    "outputId": "3db1294e-0cfe-499d-f70a-4aa60d7fb0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+------+\n",
      "| id|    race|   hp|  land|\n",
      "+---+--------+-----+------+\n",
      "|  0|     orc|70000|Mordor|\n",
      "|  1|     elf|40000| Shire|\n",
      "|  2|     elf|40000|Gondor|\n",
      "|  3|     elf|40000|Mordor|\n",
      "|  4|halfling|30000|Mordor|\n",
      "+---+--------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "land = functions.udf(lambda : random.choice(['Gondor', 'Shire', 'Mordor']))\n",
    "\n",
    "df4 = df3.withColumn('land', land())\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "7lYcX3hP-l92",
    "outputId": "0dabc3e7-734e-4936-ad86-45f85c2cf7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-----+\n",
      "|race_land|Gondor|Mordor|Shire|\n",
      "+---------+------+------+-----+\n",
      "|      orc|     3|     2|    1|\n",
      "| halfling|     2|     2|    0|\n",
      "|      elf|     3|     4|    3|\n",
      "+---------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.crosstab('race', 'land').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_J8ksUfDG4p"
   },
   "source": [
    "### Grouping\n",
    "\n",
    "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "5FPP4b54DG4r",
    "outputId": "4037a74e-a6bb-4e91-b413-09e5e3573fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  land|sum(hp)|\n",
      "+------+-------+\n",
      "| Shire| 470000|\n",
      "|Mordor| 290000|\n",
      "|Gondor| 180000|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd = df4.groupBy('land')\n",
    "gd.sum('hp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qmuADZ9DG45"
   },
   "source": [
    "GroupedData has several aggregation functions defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8sfeNPzDG48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8wZYh01DG5J"
   },
   "source": [
    "We can do several aggregations in a single step, with a number of different syntaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "UUOIYau3DG5L",
    "outputId": "588102fc-e2e3-489a-ce70-249524b51111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------+-------+\n",
      "|  land|           avg(hp)|sum(hp)|max(id)|\n",
      "+------+------------------+-------+-------+\n",
      "| Shire|           47000.0| 470000|     19|\n",
      "|Mordor|48333.333333333336| 290000|     14|\n",
      "|Gondor|           45000.0| 180000|     18|\n",
      "+------+------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd.agg(functions.avg('hp'), functions.sum('hp'), functions.max('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "cWa0ZRyFFFVc",
    "outputId": "8e5f94cf-a359-40ed-ee3b-2035d9d84538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|(id < 10)|sum(hp)|\n",
      "+---------+-------+\n",
      "|     true| 430000|\n",
      "|    false| 510000|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupBy(df4['id'] < 10).sum('hp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "mGsYfW6bFprb",
    "outputId": "bb0fb8cb-3607-4a64-a500-b78e1c3c0f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|  land|    race|sum(hp)|\n",
      "+------+--------+-------+\n",
      "| Shire|     elf| 200000|\n",
      "|Mordor|halfling|  30000|\n",
      "|Mordor|     elf| 120000|\n",
      "|Gondor|     elf|  80000|\n",
      "| Shire|     orc| 210000|\n",
      "| Shire|halfling|  60000|\n",
      "|Mordor|     orc| 140000|\n",
      "|Gondor|     orc|  70000|\n",
      "|Gondor|halfling|  30000|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupBy('land', 'race').sum('hp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R98mG-bYDG5X"
   },
   "source": [
    "### Intersections\n",
    "\n",
    "Ver much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "6t0Ljc3EDG5Z",
    "outputId": "10809196-5122-474a-d59d-f94d032ac95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   land| gold|\n",
      "+-------+-----+\n",
      "| Mordor|33765|\n",
      "|Numenor|27946|\n",
      "|Numenor|27398|\n",
      "| Mordor|36841|\n",
      "| Gondor|27882|\n",
      "| Gondor|37537|\n",
      "|Numenor|46934|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = zip(random.choices(['Mordor', 'Gondor', 'Numenor'], k=7), random.choices(range(20000, 50000), k =7))\n",
    "\n",
    "other = spark.createDataFrame(data, schema = ['land', 'gold'])\n",
    "other.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZWV8crsaG9Hf",
    "outputId": "7eaccae8-970d-4e32-e8dd-a6426d8f8887"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1019.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject [id#11L, race#12, hp(race#12) AS hp#293, <lambda>() AS land#352]\n+- LogicalRDD [id#11L, race#12], false\nand\nLogicalRDD [land#716, gold#717L], false\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1295)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1274)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-99f909e0a3fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nProject [id#11L, race#12, hp(race#12) AS hp#293, <lambda>() AS land#352]\\n+- LogicalRDD [id#11L, race#12], false\\nand\\nLogicalRDD [land#716, gold#717L], false\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "df4.join(other).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQZJwRIdDG5l"
   },
   "source": [
    "Spark refuses to do cross joins by default. To perform them, we can \n",
    "\n",
    "a) Allow then explicitly:\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "b) Specify the join criterion\n",
    "\n",
    "```python\n",
    "df4.join(new_df, on='id').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "Q7by-lvfDG5p",
    "outputId": "bf8a150a-485f-49ac-ec82-6cbe3bca0aed",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+-----+-----+\n",
      "|  land| id|    race|   hp| gold|\n",
      "+------+---+--------+-----+-----+\n",
      "|Mordor|  0|     orc|70000|33765|\n",
      "|Mordor|  0|     orc|70000|36841|\n",
      "|Mordor|  3|     elf|40000|33765|\n",
      "|Mordor|  3|     elf|40000|36841|\n",
      "|Mordor|  4|halfling|30000|33765|\n",
      "|Mordor|  4|halfling|30000|36841|\n",
      "|Mordor| 10|     elf|40000|33765|\n",
      "|Mordor| 10|     elf|40000|36841|\n",
      "|Mordor| 13|     elf|40000|33765|\n",
      "|Mordor| 13|     elf|40000|36841|\n",
      "|Mordor| 14|     orc|70000|33765|\n",
      "|Mordor| 14|     orc|70000|36841|\n",
      "|Gondor|  2|     elf|40000|37537|\n",
      "|Gondor|  8|     orc|70000|37537|\n",
      "|Gondor| 12|     elf|40000|37537|\n",
      "|Gondor| 18|halfling|30000|37537|\n",
      "+------+---+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.join(other, on='land').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "rpoE4XczIHhe",
    "outputId": "d527d2b8-a705-464b-b5e3-17d69b9f5953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+-----+-----+\n",
      "|  land| id|    race|   hp| gold|\n",
      "+------+---+--------+-----+-----+\n",
      "| Shire|  1|     elf|40000| null|\n",
      "| Shire|  5|halfling|30000| null|\n",
      "| Shire|  6|halfling|30000| null|\n",
      "| Shire|  7|     elf|40000| null|\n",
      "| Shire|  9|     elf|40000| null|\n",
      "| Shire| 11|     orc|70000| null|\n",
      "| Shire| 15|     orc|70000| null|\n",
      "| Shire| 16|     elf|40000| null|\n",
      "| Shire| 17|     orc|70000| null|\n",
      "| Shire| 19|     elf|40000| null|\n",
      "|Mordor|  0|     orc|70000|33765|\n",
      "|Mordor|  3|     elf|40000|33765|\n",
      "|Mordor|  4|halfling|30000|33765|\n",
      "|Mordor| 10|     elf|40000|33765|\n",
      "|Mordor| 13|     elf|40000|33765|\n",
      "|Mordor| 14|     orc|70000|33765|\n",
      "|Gondor|  2|     elf|40000| null|\n",
      "|Gondor|  8|     orc|70000| null|\n",
      "|Gondor| 12|     elf|40000| null|\n",
      "|Gondor| 18|halfling|30000| null|\n",
      "+------+---+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.join(other, on='land', how='left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "0XxmUIAlIbXs",
    "outputId": "85494afb-ff24-4209-ee2d-ef79fd7cf910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+-----+-----+\n",
      "|   land|  id|    race|   hp| gold|\n",
      "+-------+----+--------+-----+-----+\n",
      "|Numenor|null|    null| null|27946|\n",
      "|Numenor|null|    null| null|27398|\n",
      "|Numenor|null|    null| null|46934|\n",
      "|  Shire|   1|     elf|40000| null|\n",
      "|  Shire|   5|halfling|30000| null|\n",
      "|  Shire|   6|halfling|30000| null|\n",
      "|  Shire|   7|     elf|40000| null|\n",
      "|  Shire|   9|     elf|40000| null|\n",
      "|  Shire|  12|     elf|40000| null|\n",
      "|  Shire|  13|     elf|40000| null|\n",
      "|  Shire|  14|     orc|70000| null|\n",
      "|  Shire|  16|     elf|40000| null|\n",
      "|  Shire|  17|     orc|70000| null|\n",
      "|  Shire|  19|     elf|40000| null|\n",
      "| Mordor|   0|     orc|70000|33765|\n",
      "| Mordor|   0|     orc|70000|36841|\n",
      "| Mordor|   3|     elf|40000|33765|\n",
      "| Mordor|   3|     elf|40000|36841|\n",
      "| Mordor|   4|halfling|30000|33765|\n",
      "| Mordor|   4|halfling|30000|36841|\n",
      "+-------+----+--------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.join(other, on='land', how='outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "IIbizsHvJgWD",
    "outputId": "3e24650a-3422-4563-8053-47d3e3fc5879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+-----+-------+-----+\n",
      "|  id|    race|   hp| land|   land| gold|\n",
      "+----+--------+-----+-----+-------+-----+\n",
      "|null|    null| null| null|Numenor|27946|\n",
      "|null|    null| null| null|Numenor|27398|\n",
      "|null|    null| null| null|Numenor|46934|\n",
      "|   1|     elf|40000|Shire|   null| null|\n",
      "|   5|halfling|30000|Shire|   null| null|\n",
      "+----+--------+-----+-----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.join(other, on = df4['land'] == other['land'], how='outer').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "bJeVbvQ7KQ2k",
    "outputId": "ef3e2021-22b9-4975-bc69-eb357bb80fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+------+-------+-----+\n",
      "| id|    race|   hp|  land|   land| gold|\n",
      "+---+--------+-----+------+-------+-----+\n",
      "|  0|     orc|70000| Shire|   null| null|\n",
      "|  1|     elf|40000|Gondor|Numenor|46934|\n",
      "|  2|     elf|40000|Gondor|Numenor|46934|\n",
      "|  3|     elf|40000|Mordor|Numenor|46934|\n",
      "|  4|halfling|30000|Mordor| Mordor|33765|\n",
      "|  4|halfling|30000|Mordor| Mordor|36841|\n",
      "|  4|halfling|30000|Mordor| Gondor|37537|\n",
      "|  4|halfling|30000|Mordor|Numenor|46934|\n",
      "|  5|halfling|30000|Gondor| Mordor|33765|\n",
      "|  5|halfling|30000|Gondor| Mordor|36841|\n",
      "|  5|halfling|30000|Gondor| Gondor|37537|\n",
      "|  5|halfling|30000|Gondor|Numenor|46934|\n",
      "|  6|halfling|30000|Mordor| Mordor|33765|\n",
      "|  6|halfling|30000|Mordor| Mordor|36841|\n",
      "|  6|halfling|30000|Mordor| Gondor|37537|\n",
      "|  6|halfling|30000|Mordor|Numenor|46934|\n",
      "|  7|     elf|40000|Gondor|Numenor|46934|\n",
      "|  8|     orc|70000|Gondor|   null| null|\n",
      "|  9|     elf|40000| Shire|Numenor|46934|\n",
      "| 10|     elf|40000| Shire|Numenor|46934|\n",
      "+---+--------+-----+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.join(other, on = df4['hp'] < other['gold'], how='outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpPG9c2wDG5y"
   },
   "source": [
    "#### Digression\n",
    "\n",
    "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
    "\n",
    "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBeip8ziDG57"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6LhqmJ2DG6I"
   },
   "source": [
    "We can erase it with `unpersist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdoyS7cKDG6M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZnyp2pQDG6c"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each creature's hitpoints for their location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oULOu6jmDG6j",
    "outputId": "1b5f392b-0bca-4d28-c56b-fcda526882ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, race: string, hp: int, land: string, (hp * 2): int, uncuarto: double]"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.select('*',\n",
    "           df4['hp'] * 2,\n",
    "           (df4['id'] / 4).alias('uncuarto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-tCefztQbNK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75csIlurDG6z"
   },
   "source": [
    "1) Calculate the mean and std of hitpoints for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "AM0LueibDG62",
    "outputId": "a65842b4-d838-4b96-aec7-8c15ecfe95f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+\n",
      "|  land|            avg_hp|            std_hp|\n",
      "+------+------------------+------------------+\n",
      "| Shire|57142.857142857145|16035.674514745464|\n",
      "|Mordor|41666.666666666664|14719.601443879743|\n",
      "|Gondor| 41428.57142857143|13451.854182690986|\n",
      "+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = df4.groupby('land').agg(functions.avg('hp').alias('avg_hp'), \n",
    "                                functions.stddev('hp').alias('std_hp'))\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBMmvA1lDG7A"
   },
   "source": [
    "2) Annotate each creature with the stats corresponding to their location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "U_K-op6ODG7F",
    "outputId": "119d0928-ab19-4e00-af20-1a6a05f47176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+-----+-------+-------+\n",
      "| land| id|    race|   hp| avg_hp| std_hp|\n",
      "+-----+---+--------+-----+-------+-------+\n",
      "|Shire|  1|     elf|40000|47500.0|15000.0|\n",
      "|Shire|  6|halfling|30000|47500.0|15000.0|\n",
      "|Shire|  9|     elf|40000|47500.0|15000.0|\n",
      "|Shire| 11|     orc|70000|47500.0|15000.0|\n",
      "|Shire| 16|     elf|40000|47500.0|15000.0|\n",
      "+-----+---+--------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotated = df4.join(stats, on='land')\n",
    "annotated.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4ZyrPfBDG7R"
   },
   "source": [
    "3) Calculate the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "JbVBaSgXDG7V",
    "outputId": "eb7c07a4-13a6-4ddf-9057-326803dd94c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+-----+------------------+------------------+--------------------+\n",
      "|  land| id|    race|   hp|            avg_hp|            std_hp|                   z|\n",
      "+------+---+--------+-----+------------------+------------------+--------------------+\n",
      "| Shire|  0|     orc|70000|           47000.0| 16363.91694484477|  1.4055314554285756|\n",
      "| Shire|  2|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
      "| Shire| 10|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
      "| Shire| 12|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
      "|Mordor|  1|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
      "|Mordor|  6|halfling|30000|48333.333333333336|17224.014243685084| -1.0644053746097526|\n",
      "|Mordor|  8|     orc|70000|48333.333333333336|17224.014243685084|  1.2579336245387982|\n",
      "|Mordor|  9|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
      "|Mordor| 11|     orc|70000|48333.333333333336|17224.014243685084|  1.2579336245387982|\n",
      "|Mordor| 16|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
      "|Mordor| 18|halfling|30000|48333.333333333336|17224.014243685084| -1.0644053746097526|\n",
      "|Mordor| 19|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
      "|Gondor|  3|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
      "|Gondor|  4|halfling|30000|           45000.0|17320.508075688773| -0.8660254037844387|\n",
      "|Gondor|  5|halfling|30000|           45000.0|17320.508075688773| -0.8660254037844387|\n",
      "|Gondor|  7|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
      "|Gondor| 13|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
      "|Gondor| 14|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
      "|Gondor| 15|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
      "|Gondor| 17|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
      "+------+---+--------+-----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def zscore(xi, mean, std):\n",
    "  return (xi - mean) / std \n",
    "\n",
    "z_udf = functions.udf(zscore, returnType=types.DoubleType())\n",
    "\n",
    "annotated.withColumn('z', z_udf('hp', 'avg_hp', 'std_hp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mmVFG5xsVW08",
    "outputId": "47f92335-2593-4c41-c892-25535d3e258e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, race: string, land: string, hp: int, z: double]"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated.select('id', \n",
    "                 'race',\n",
    "                 'land',\n",
    "                 'hp',\n",
    "                 z_udf('hp', 'avg_hp', 'std_hp').alias('z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UmqyeVMKVpds",
    "outputId": "d8d6dc03-9471-41f7-c28a-2c2c967d9c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, race: string, land: string, hp: int, z: double]"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated.select('id', \n",
    "                 'race',\n",
    "                 'land',\n",
    "                 'hp',\n",
    "                 ((annotated['hp'] - annotated['avg_hp']) / annotated['std_hp']).alias('z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3snR7Y5hDG7k"
   },
   "source": [
    "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kVGSJ_ZDG7l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn8fSPdZDG7t"
   },
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "DlwoNIBiDG7u",
    "outputId": "bb11adb9-5d15-43ba-9cf3-933a834b942f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+-----+-----+\n",
      "|   land|  id|    race|   hp| gold|\n",
      "+-------+----+--------+-----+-----+\n",
      "|Numenor|null|    null| null|27946|\n",
      "|Numenor|null|    null| null|27398|\n",
      "|Numenor|null|    null| null|46934|\n",
      "|  Shire|   1|     elf|40000| null|\n",
      "|  Shire|   5|halfling|30000| null|\n",
      "|  Shire|   6|halfling|30000| null|\n",
      "|  Shire|   7|     elf|40000| null|\n",
      "|  Shire|   9|     elf|40000| null|\n",
      "|  Shire|  11|     orc|70000| null|\n",
      "|  Shire|  15|     orc|70000| null|\n",
      "|  Shire|  16|     elf|40000| null|\n",
      "|  Shire|  17|     orc|70000| null|\n",
      "|  Shire|  19|     elf|40000| null|\n",
      "| Mordor|   0|     orc|70000|33765|\n",
      "| Mordor|   0|     orc|70000|36841|\n",
      "| Mordor|   3|     elf|40000|33765|\n",
      "| Mordor|   3|     elf|40000|36841|\n",
      "| Mordor|   4|halfling|30000|33765|\n",
      "| Mordor|   4|halfling|30000|36841|\n",
      "| Mordor|  10|     elf|40000|33765|\n",
      "+-------+----+--------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_nulls = df4.join(other, on='land', how='outer')\n",
    "with_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "-QlHmFraaPiu",
    "outputId": "06fa96a2-970b-42fd-e7e6-69f83e23d07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+-----+-----+\n",
      "|  land| id|    race|   hp| gold|\n",
      "+------+---+--------+-----+-----+\n",
      "|Mordor|  0|     orc|70000|33765|\n",
      "|Mordor|  0|     orc|70000|36841|\n",
      "|Mordor|  3|     elf|40000|33765|\n",
      "|Mordor|  3|     elf|40000|36841|\n",
      "|Mordor|  4|halfling|30000|33765|\n",
      "|Mordor|  4|halfling|30000|36841|\n",
      "|Mordor| 10|     elf|40000|33765|\n",
      "|Mordor| 10|     elf|40000|36841|\n",
      "|Mordor| 13|     elf|40000|33765|\n",
      "|Mordor| 13|     elf|40000|36841|\n",
      "|Mordor| 14|     orc|70000|33765|\n",
      "|Mordor| 14|     orc|70000|36841|\n",
      "|Gondor|  2|     elf|40000|27882|\n",
      "|Gondor|  2|     elf|40000|37537|\n",
      "|Gondor|  8|     orc|70000|27882|\n",
      "|Gondor|  8|     orc|70000|37537|\n",
      "|Gondor| 12|     elf|40000|27882|\n",
      "|Gondor| 12|     elf|40000|37537|\n",
      "|Gondor| 18|halfling|30000|27882|\n",
      "|Gondor| 18|halfling|30000|37537|\n",
      "+------+---+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_nulls.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "54IH1cr7aiqi",
    "outputId": "6d62b829-6d0c-48fe-caad-e526a42832b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+-----+----+\n",
      "|  land| id|race|   hp|gold|\n",
      "+------+---+----+-----+----+\n",
      "| Shire|  0| orc|70000|null|\n",
      "| Shire|  2| elf|40000|null|\n",
      "| Shire| 10| elf|40000|null|\n",
      "| Shire| 12| elf|40000|null|\n",
      "|Mordor|  1| elf|40000|null|\n",
      "+------+---+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_nulls.dropna(subset='race').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "wBBURgd_appj",
    "outputId": "ed617501-36eb-4129-ed28-d792e6984401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+-----+-----+\n",
      "|   land|  id|    race|   hp| gold|\n",
      "+-------+----+--------+-----+-----+\n",
      "|Numenor|null|    null| null|27946|\n",
      "|Numenor|null|    null| null|27398|\n",
      "|Numenor|null|    null| null|46934|\n",
      "|  Shire|   1|     elf|40000| null|\n",
      "|  Shire|   5|halfling|30000| null|\n",
      "+-------+----+--------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_nulls.dropna(how='all', subset=['race', 'land']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmmzCaYnDG75"
   },
   "source": [
    "## SQL querying\n",
    "\n",
    "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hT0PkV6XDG79",
    "outputId": "a56e8fe4-3e6c-4628-f2a2-1e10e9507eba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 27))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: annotated; line 3 pos 18\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'annotated' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 53 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-6bc392cb2bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spark.sql('''SELECT id,\n\u001b[1;32m      2\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_hp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_hp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m              FROM annotated''')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: annotated; line 3 pos 18'"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT id,\n",
    "                    ((hp - avg_hp) / std_hp) AS z\n",
    "             FROM annotated''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XibCAlOb5LE"
   },
   "outputs": [],
   "source": [
    "annotated.registerTempTable('annotated_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5LlrxOhDG8L"
   },
   "source": [
    "Once registered, we can perform queries as complex as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "J9btRD25DG8M",
    "outputId": "91b06a76-6157-4b0c-be12-3b807d017537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  z|\n",
      "+---+-------------------+\n",
      "|  1|               -0.5|\n",
      "|  5|-1.1666666666666667|\n",
      "|  6|-1.1666666666666667|\n",
      "|  7|               -0.5|\n",
      "|  9|               -0.5|\n",
      "+---+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''SELECT id,\n",
    "                             ((hp - avg_hp) / std_hp) AS z\n",
    "                      FROM annotated_table''')\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AH86T4G1DG8X"
   },
   "source": [
    "## Interoperation with Pandas\n",
    "\n",
    "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "U8gKF6avdWal",
    "outputId": "2c7f066f-b2b6-496f-eb94-be8178f96d89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1595056035824'),\n",
       " ('spark.driver.port', '35545'),\n",
       " ('spark.driver.host', '07272e473bf8'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('someconfigkey', 'someconfigvalue')]"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "WUHpNVhuDG8g",
    "outputId": "1acedec4-0e6c-4535-e39b-9631279281bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>land</th>\n",
       "      <th>id</th>\n",
       "      <th>race</th>\n",
       "      <th>hp</th>\n",
       "      <th>avg_hp</th>\n",
       "      <th>std_hp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shire</td>\n",
       "      <td>0</td>\n",
       "      <td>orc</td>\n",
       "      <td>70000</td>\n",
       "      <td>47000.0</td>\n",
       "      <td>16363.916945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shire</td>\n",
       "      <td>2</td>\n",
       "      <td>elf</td>\n",
       "      <td>40000</td>\n",
       "      <td>47000.0</td>\n",
       "      <td>16363.916945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shire</td>\n",
       "      <td>10</td>\n",
       "      <td>elf</td>\n",
       "      <td>40000</td>\n",
       "      <td>47000.0</td>\n",
       "      <td>16363.916945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    land  id race     hp   avg_hp        std_hp\n",
       "0  Shire   0  orc  70000  47000.0  16363.916945\n",
       "1  Shire   2  elf  40000  47000.0  16363.916945\n",
       "2  Shire  10  elf  40000  47000.0  16363.916945"
      ]
     },
     "execution_count": 163,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = annotated.toPandas()\n",
    "pd_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "0ImEmCEGdrq4",
    "outputId": "f3d9461a-f259-4a14-a8ba-7c7774d056b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]),\n",
       " array([30000., 34000., 38000., 42000., 46000., 50000., 54000., 58000.,\n",
       "        62000., 66000., 70000.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(pd_df['hp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "copDljPYd7op",
    "outputId": "beac9f32-17c5-4408-a107-461d664a7534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[land: string, id: bigint, race: string, hp: bigint, avg_hp: double, std_hp: double]"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Je_NF-zVDG80"
   },
   "source": [
    "## Writing out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kG1YnS75fhR8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39zOmZO9DG82"
   },
   "outputs": [],
   "source": [
    "annotated.write.csv('annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "zqYH4Z2xgYLG",
    "outputId": "1d74f44e-da61-4b32-f396-6c37c181b659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shire,0,orc,70000,47000.0,16363.91694484477\n",
      "Shire,2,elf,40000,47000.0,16363.91694484477\n",
      "Shire,10,elf,40000,47000.0,16363.91694484477\n",
      "Shire,12,elf,40000,47000.0,16363.91694484477\n"
     ]
    }
   ],
   "source": [
    "!head annotated.csv/part-00033-51449480-3d2e-49a4-8064-f9ed1b86d339-c000.csv  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjEk-9RBDG9C"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "1. Total ticket amounts per origin\n",
    "2. Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "_hmTZQVIDG9E",
    "outputId": "89137b4c-ddbd-4ef9-9eea-f71d7e0c810a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\n",
      "79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0\n",
      "79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0\n",
      "79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0\n",
      "79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0\n"
     ]
    }
   ],
   "source": [
    "!zcat coupon150720.csv.gz | head -n 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nkk7yA1uDG9a"
   },
   "source": [
    "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
    "\n",
    "Remember, you want to calculate:\n",
    "\n",
    "Total ticket amounts per origin\n",
    "\n",
    "Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UpAgVqIcDG9c",
    "outputId": "335d2ffc-6076-431f-db73-72a9fddfcd0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tkt_number: bigint, cpn_number: int, origin: string, dest: string, carrier: string, amount: float]"
      ]
     },
     "execution_count": 175,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons = spark.sql('''SELECT CAST(_c0 AS BIGINT) AS tkt_number,\n",
    "                              CAST(_c1 AS INT) AS cpn_number,\n",
    "                              _c2 AS origin,\n",
    "                              _c3 AS dest,\n",
    "                              _c4 AS carrier,\n",
    "                              CAST(_c6 AS FLOAT) AS amount\n",
    "                       FROM csv.`coupon150720.csv.gz`''')\n",
    "\n",
    "coupons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3geIOiRMDG9n"
   },
   "source": [
    "2) Total ticket amounts per origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "REAxdFlXDG9p",
    "outputId": "109e8d35-d025-42af-bc0a-a5f399c21fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|       sum(amount)|\n",
      "+------+------------------+\n",
      "|   PMI| 40547.17005729675|\n",
      "|   YUL|284.44000244140625|\n",
      "|   HEL| 8195.760055541992|\n",
      "|   SXB| 264.4599914550781|\n",
      "|   UIO| 8547.599964141846|\n",
      "+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coupons.where(coupons['dest'] == 'MAD')\\\n",
    "       .groupby('origin')\\\n",
    "       .sum('amount').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzISd6WvDG9u"
   },
   "source": [
    "3) Top 10 Airlines by average amount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "XnCQ4pALDG9y",
    "outputId": "9479dece-1b6d-4c49-9f23-6aaeaec83352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|carrier|       avg(amount)|\n",
      "+-------+------------------+\n",
      "|     V0| 5418.098665364583|\n",
      "|     AC|  740.619985961914|\n",
      "|     KE| 688.5261500431941|\n",
      "|     SV|  553.174259916265|\n",
      "|     OB| 535.5044420030382|\n",
      "|     AR| 513.5304808843704|\n",
      "|     AV| 450.1950941518613|\n",
      "|     AM| 440.7342111687911|\n",
      "|     C2| 397.8699951171875|\n",
      "|     LA|379.95370341954606|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coupons.where(coupons['dest'] == 'MAD')\\\n",
    "       .groupBy('carrier')\\\n",
    "       .avg('amount')\\\n",
    "       .sort('avg(amount)', ascending=False)\\\n",
    "       .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCyX_XkPDG97"
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
    "\n",
    "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
    "\n",
    "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "02-SparkSQL-DataFrames.inclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
