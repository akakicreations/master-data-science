{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "02-SparkSQL-DataFrames.inclass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W1D1mPbDGvy",
        "colab_type": "text"
      },
      "source": [
        "# SparkSQL and DataFrames \n",
        "\n",
        "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdESfq7rDGv3",
        "colab_type": "text"
      },
      "source": [
        "## RDDs, DataSets, and DataFrames\n",
        "\n",
        "RDDs are the original interface for Spark programming.\n",
        "\n",
        "DataFrames were introduced in 1.3\n",
        "\n",
        "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9bk6mqqDGv6",
        "colab_type": "text"
      },
      "source": [
        "### Advantages of DataFrames:\n",
        "\n",
        "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
        "\n",
        "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2k1UFbFDGv_",
        "colab_type": "text"
      },
      "source": [
        "## SparkSQL and DataFrames \n",
        "\n",
        "\n",
        "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
        "\n",
        "From https://spark.apache.org/docs/2.4.4/sql-programming-guide.html\n",
        "\n",
        "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgoFtnisDGwE",
        "colab_type": "text"
      },
      "source": [
        "### The pyspark.sql module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENLGO-dFDGwH",
        "colab_type": "text"
      },
      "source": [
        "Important classes of Spark SQL and DataFrames:\n",
        "\n",
        "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
        "\n",
        "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
        "\n",
        "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
        "\n",
        "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
        "\n",
        "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
        "\n",
        "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
        "\n",
        "* `pyspark.sql.types` List of data types available.\n",
        "\n",
        "* `pyspark.sql.Window` For working with window functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTx0dOg0DGwK",
        "colab_type": "text"
      },
      "source": [
        "http://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html\n",
        "\n",
        "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocdELJEWDGwQ",
        "colab_type": "text"
      },
      "source": [
        "## SparkSession\n",
        "\n",
        "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
        "\n",
        "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5To3q5kDGwW",
        "colab_type": "text"
      },
      "source": [
        "### If you are running this notebook in Google Colab\n",
        "\n",
        "Copy the following to a code cell and run it. It will install and set up Spark for you.\n",
        "\n",
        "```python\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp6qy7qVDGwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n",
        " \n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        " \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.ui.port\", \"4050\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-vzNodrENEI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33eyL9fhBBHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "34fc6449-a549-497a-eaae-d2aaf021dec5"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -qq ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-20 16:31:59--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.206.168.28, 54.208.57.0, 34.194.108.77, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.206.168.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  38.5MB/s    in 0.3s    \n",
            "\n",
            "2020-07-20 16:31:59 (38.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cifaGXilAOnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e907a3e-587a-4340-c320-5b6503e79ecb"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://74b7ab714b4f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ks5c3ncDGwu",
        "colab_type": "text"
      },
      "source": [
        "#### Passing other options to spark session:\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTcj4eFoDGww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.config('someconfigkey', 'someconfigvalue').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH8oAEzHDGw_",
        "colab_type": "text"
      },
      "source": [
        "We can check option values in the resulting session like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrFX2JY0DGxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "76b307ff-9435-4657-ee26-f23e81611636"
      },
      "source": [
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.id', 'local-1595056035824'),\n",
              " ('spark.driver.port', '35545'),\n",
              " ('spark.driver.host', '07272e473bf8'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('someconfigkey', 'someconfigvalue')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0wsa4JfDGxX",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-7764utDGxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "534559ba-6a48-4a27-9ae4-66aac4f475a6"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "n = 20\n",
        "creatures = random.choices(['elf', 'orc', 'halfling'], k=n)\n",
        "\n",
        "rows = list(zip(range(n), creatures))\n",
        "rows"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'orc'),\n",
              " (1, 'elf'),\n",
              " (2, 'elf'),\n",
              " (3, 'elf'),\n",
              " (4, 'halfling'),\n",
              " (5, 'halfling'),\n",
              " (6, 'halfling'),\n",
              " (7, 'elf'),\n",
              " (8, 'orc'),\n",
              " (9, 'elf'),\n",
              " (10, 'elf'),\n",
              " (11, 'orc'),\n",
              " (12, 'elf'),\n",
              " (13, 'elf'),\n",
              " (14, 'orc'),\n",
              " (15, 'orc'),\n",
              " (16, 'elf'),\n",
              " (17, 'orc'),\n",
              " (18, 'halfling'),\n",
              " (19, 'elf')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0mCom91GX1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93893260-9e2a-4317-ce50-19ad5ed3caec"
      },
      "source": [
        "df = spark.createDataFrame(rows)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_1: bigint, _2: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKibVa6nGkj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a12497d6-e09b-48cd-a3d6-ff7c632a05da"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| _1|      _2|\n",
            "+---+--------+\n",
            "|  0|     orc|\n",
            "|  1|     elf|\n",
            "|  2|     elf|\n",
            "|  3|     elf|\n",
            "|  4|halfling|\n",
            "+---+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBtsgIZyGvpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "483e2cb0-0ec5-4924-f26b-79c5b41266e7"
      },
      "source": [
        "df.rdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3X-53gqG0aY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2269a883-7a9e-410e-bc8e-f28611693a56"
      },
      "source": [
        "df.rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=0, _2='orc'),\n",
              " Row(_1=1, _2='elf'),\n",
              " Row(_1=2, _2='elf'),\n",
              " Row(_1=3, _2='elf'),\n",
              " Row(_1=4, _2='halfling')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu1V44ImHIzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "572a861a-0348-43a2-e964-799af4ac149f"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "\n",
        "\n",
        "a_row = Row(x=1, y=0.5)\n",
        "a_row"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(x=1, y=0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luBPyhMCHXgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d3785d5-c98e-4b59-9ae1-ea56371f30ea"
      },
      "source": [
        "a_row['x']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH8p_KbXDGxq",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "* From RDDs\n",
        "* from Hive tables\n",
        "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZK7TQRhHhQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad6dd3d0-1e69-4002-eafd-f9f3f1ed81ec"
      },
      "source": [
        "df = spark.createDataFrame(rows, schema=['id', 'race'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_BtZF00DGxr",
        "colab_type": "text"
      },
      "source": [
        "#### From RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J99dtfrgDGxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "85ccf369-be2d-49c0-d034-5a611e7df7b8"
      },
      "source": [
        "!wget -v http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-18 07:09:55--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5777367 (5.5M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.51M  1.33MB/s    in 4.1s    \n",
            "\n",
            "2020-07-18 07:09:59 (1.33 MB/s) - ‘shakespeare.txt’ saved [5777367/5777367]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1zIdJtSIHUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3a774b04-fefc-4737-93b2-9ba01954129f"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation(line):\n",
        "  return re.sub('[^a-z0-9 ]', '', line.lower())\n",
        "\n",
        "shakespeare = spark.sparkContext.textFile('shakespeare.txt')\n",
        "\n",
        "freqs = shakespeare.map(remove_punctuation)\\\n",
        "                   .flatMap(str.split)\\\n",
        "                   .map(lambda word: (word, 1))\\\n",
        "                   .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "freqs.take(5)          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 107),\n",
              " ('of', 18811),\n",
              " ('shakespeare', 13),\n",
              " ('this', 7177),\n",
              " ('ebook', 14)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdaIZM0vJLFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a649ac8-8563-478a-dbbf-e59a9845d223"
      },
      "source": [
        "freqs_df = spark.createDataFrame(freqs, schema=['word', 'times'])\n",
        "freqs_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[word: string, times: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwwEMWimDGx_",
        "colab_type": "text"
      },
      "source": [
        "### Inferring and specifying schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7rsfbTHDGyB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "daa14891-8bab-4da6-b792-287859a6c251"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEXYNzrfDGyT",
        "colab_type": "text"
      },
      "source": [
        "#### Fully specifying a schema\n",
        "\n",
        "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WySUSjj8DGyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf7da70b-7456-4c1f-9663-5ee297cddd0a"
      },
      "source": [
        "from pyspark.sql import types\n",
        "\n",
        "types.IntegerType()\n",
        "types.FloatType()\n",
        "types.DoubleType()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DoubleType"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y71I3VLRKjAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33d0ef2f-029f-4a77-ff9e-192923941718"
      },
      "source": [
        "coord_type = types.StructType(fields=[types.StructField('x', types.FloatType()), \n",
        "                                      types.StructField('y', types.FloatType())])\n",
        "\n",
        "coord_type"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(x,FloatType,true),StructField(y,FloatType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUyct_-pDGyi",
        "colab_type": "text"
      },
      "source": [
        "#### From csv files\n",
        "\n",
        "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp8-7C4RLxjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1892c471-93cd-4d46-bd4b-745eaa3c001f"
      },
      "source": [
        "!zcat coupon150720.csv.gz | head -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\n",
            "79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0\n",
            "79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0\n",
            "79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0\n",
            "79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxtUGp3SDGyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "632b63b7-420f-4692-f71a-f9a13331655c"
      },
      "source": [
        "coupons = spark.read.csv('coupon150720.csv.gz', inferSchema=True)\n",
        "coupons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: bigint, _c1: int, _c2: string, _c3: string, _c4: string, _c5: string, _c6: double, _c7: string, _c8: int, _c9: string, _c10: string, _c11: string, _c12: int, _c13: string, _c14: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-nPIx9GMaM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12fbd2ac-f61b-4b2f-d9c6-4807290c799b"
      },
      "source": [
        "coupons = spark.sql('''SELECT _c0 AS tkt_number, \n",
        "                              _c2 AS origin,  \n",
        "                              _c3 AS dest\n",
        "                       FROM csv.`coupon150720.csv.gz`''')\n",
        "coupons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[tkt_number: string, origin: string, dest: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a-YvdT7DGy8",
        "colab_type": "text"
      },
      "source": [
        "#### From other types of data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiBDDssgDGy_",
        "colab_type": "text"
      },
      "source": [
        "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIlE_rnqDGzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "056067bf-7b84-4873-fd50-5d7e6a277b36"
      },
      "source": [
        "spark.read.parquet\n",
        "spark.read.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrameReader.json of <pyspark.sql.readwriter.DataFrameReader object at 0x7febe68cbf98>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8RYleWtDGzR",
        "colab_type": "text"
      },
      "source": [
        "### Basic operations with DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lFsKLaRDGzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "f7c7ad1d-b551-40c4-8b01-64bdb0f964ed"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id|    race|\n",
            "+---+--------+\n",
            "|  0|     orc|\n",
            "|  1|     elf|\n",
            "|  2|     elf|\n",
            "|  3|     elf|\n",
            "|  4|halfling|\n",
            "+---+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzM-XgtjPKp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "00231235-78e7-4426-983f-f9843794e80c"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmNJ7fWcDGzl",
        "colab_type": "text"
      },
      "source": [
        "### Filtering and selecting\n",
        "\n",
        "Syntax inspired in SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_4lY-OPDGzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adc5b3b0-8eae-46ac-e3d1-b854d50a5b83"
      },
      "source": [
        "df.select('id')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIxWKdOJPdVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8bf5353-6c3f-40a6-de5d-885b65e0a43c"
      },
      "source": [
        "coupons.select('tkt_number',\n",
        "               'origin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[tkt_number: string, origin: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FHYtdv_Pk_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d878c239-738d-4708-de44-7048f0fcb0bf"
      },
      "source": [
        "coupons[['tkt_number', 'origin']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[tkt_number: string, origin: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUc2ltu3DGz3",
        "colab_type": "text"
      },
      "source": [
        "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FpeJsdKDGz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de6d3f84-403f-43d0-e4ca-e3ef64dba4f5"
      },
      "source": [
        "df['id']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'id'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljG0HQTuQv7e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aca705ae-9abd-4613-d5bb-f536369e36d6"
      },
      "source": [
        "df[['id']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsNqmVEqRKeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "ef948882-8d0b-4eab-cd80-190d67150d6e"
      },
      "source": [
        "df['id'].take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a80c23594027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGI3H9pZRVD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d185b859-6a91-4c63-98a0-f42ece1532ef"
      },
      "source": [
        "df['id'] > 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'(id > 5)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhULM-y5DG0L",
        "colab_type": "text"
      },
      "source": [
        "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "JBeFv4CtDG0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "1688a508-54c7-481c-9775-7055e7bb9eb7"
      },
      "source": [
        "df.where(df['id'] < 5).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id|    race|\n",
            "+---+--------+\n",
            "|  0|     orc|\n",
            "|  1|     elf|\n",
            "|  2|     elf|\n",
            "|  3|     elf|\n",
            "|  4|halfling|\n",
            "+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUXsyIpR2f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cf6fe25-26e7-4020-c470-f834682af9e5"
      },
      "source": [
        "# These return the same\n",
        "df.select(df['id'])\n",
        "df.select('id')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HC-tL1GSNqh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eef857de-e22e-46e1-a740-0be160fb2bf5"
      },
      "source": [
        "df.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'id'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK14HOlXSWPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7f1be8e0-6615-436b-c6a3-3b3bcaf32142"
      },
      "source": [
        "'id' < 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4509c9174cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'id'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQNrKmRJSD9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "fcdc4edb-56a9-4e0e-d0d3-90f8e6e8b9d1"
      },
      "source": [
        "df.where('id' < 5).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-fb69e4b2adb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK9HbYirDG0c",
        "colab_type": "text"
      },
      "source": [
        "`where` is exactly synonimous with `filter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRJ9QVT_DG0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "2b5de540-a1eb-44b5-f610-3be5ee1effe0"
      },
      "source": [
        "df.filter(df['id'] < 5).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id|    race|\n",
            "+---+--------+\n",
            "|  0|     orc|\n",
            "|  1|     elf|\n",
            "|  2|     elf|\n",
            "|  3|     elf|\n",
            "|  4|halfling|\n",
            "+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nE5MJnyDG0q",
        "colab_type": "text"
      },
      "source": [
        "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZUOMnQeDG04",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Extract all employee ids which correspond to orcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wYocjIDG08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "bec5b85e-d1e6-4463-f46e-04e4eb7b428c"
      },
      "source": [
        "df.where(df['race'] == 'orc').select('id').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  8|\n",
            "| 11|\n",
            "| 14|\n",
            "| 15|\n",
            "| 17|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKVbQfHnUBRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6fa14df0-7078-47c8-dcd6-3afd1b1577f2"
      },
      "source": [
        "df[df['race'] == 'orc'][['id']].show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  8|\n",
            "| 11|\n",
            "| 14|\n",
            "| 15|\n",
            "| 17|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8dibNBPDG1V",
        "colab_type": "text"
      },
      "source": [
        "### Adding columns\n",
        "\n",
        "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "hNwmjKs1DG1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "e8b7672a-e74e-4398-d606-b6d429a9b338"
      },
      "source": [
        "df['number'] = df['id'] ** 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-361a611c0489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHC-hF7Uija",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0a9f747-bb14-468c-d5b8-ed45644f48cb"
      },
      "source": [
        "df.withColumn('number', df['id'] ** 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string, number: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpQRwS9VU7oB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30ad49d4-293e-44d3-d09f-1e60d333ef5e"
      },
      "source": [
        "a = 'and'\n",
        "a += 'oi'\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'andoi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xtR7IlODG1m",
        "colab_type": "text"
      },
      "source": [
        "### User defined functions\n",
        "\n",
        "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
        "\n",
        "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-flictADG1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "86757186-52c5-4625-ba9d-8ed3566bb02a"
      },
      "source": [
        "from math import log1p \n",
        "\n",
        "log1p(df['id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8b7d6f064158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog1p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not Column"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukTHoL0aDG18",
        "colab_type": "text"
      },
      "source": [
        "This errors out because \n",
        "\n",
        "```python\n",
        "math.log1p\n",
        "```\n",
        "\n",
        "is not a udf: it doesn't know how to work with strings or Column objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "f6i-i2D4DG1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "f7de4de1-6933-4c0d-bd41-c0a9341f6318"
      },
      "source": [
        "from pyspark.sql import functions\n",
        "\n",
        "df.withColumn('newcolumn', functions.log1p(df['id'])).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+------------------+\n",
            "| id|    race|         newcolumn|\n",
            "+---+--------+------------------+\n",
            "|  0|     orc|               0.0|\n",
            "|  1|     elf|0.6931471805599453|\n",
            "|  2|     elf|1.0986122886681096|\n",
            "|  3|     elf|1.3862943611198906|\n",
            "|  4|halfling|1.6094379124341003|\n",
            "+---+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grksVgSsDG2P",
        "colab_type": "text"
      },
      "source": [
        "But we can transform it into a udf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlc4q0OPDG2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6a329933-dabe-45a5-8e15-5262d6d3d9b4"
      },
      "source": [
        "my_random_udf = functions.udf(lambda word: word[-1].upper())\n",
        "df.withColumn('newcolumn', my_random_udf(df['race'])).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+---------+\n",
            "| id|    race|newcolumn|\n",
            "+---+--------+---------+\n",
            "|  0|     orc|        C|\n",
            "|  1|     elf|        F|\n",
            "|  2|     elf|        F|\n",
            "|  3|     elf|        F|\n",
            "|  4|halfling|        G|\n",
            "+---+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Jh2l5CDG2i",
        "colab_type": "text"
      },
      "source": [
        "We can do the same with any function we dream up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmu8-CHcDG2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d04c0490-a19b-4dd8-f1e2-42bb646abe91"
      },
      "source": [
        "def absurd_function(a, b):\n",
        "  return (a + len(b)) ** 2\n",
        "\n",
        "my_absurd_udf = functions.udf(absurd_function)\n",
        "\n",
        "df2 = df.withColumn('absur', my_absurd_udf('id', 'race'))\n",
        "df2.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+\n",
            "| id|    race|absur|\n",
            "+---+--------+-----+\n",
            "|  0|     orc|    9|\n",
            "|  1|     elf|   16|\n",
            "|  2|     elf|   25|\n",
            "|  3|     elf|   36|\n",
            "|  4|halfling|  144|\n",
            "+---+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGeSfRHEDG20",
        "colab_type": "text"
      },
      "source": [
        "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5AdMHftDG23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "6a7a6d3e-379f-4577-8d48-423ddba52847"
      },
      "source": [
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            " |-- absur: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReOOEMjtDG3F",
        "colab_type": "text"
      },
      "source": [
        "Think about this function: what is its return type?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij6uRD7hDG3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incognito(a, b):\n",
        "  return a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0O6DFA4Hd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "42e138aa-56ba-4fc4-f468-59a61e96c8fd"
      },
      "source": [
        "def absurd_function(a, b):\n",
        "  return (a + len(b)) ** 2\n",
        "\n",
        "my_absurd_udf = functions.udf(absurd_function, returnType=types.IntegerType())\n",
        "\n",
        "df2 = df.withColumn('absur', my_absurd_udf('id', 'race'))\n",
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            " |-- absur: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmiZM-yY6Ix-",
        "colab_type": "text"
      },
      "source": [
        "If we have a column that is not the desired type, we can convert it with `cast`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLDhJ5IY6PFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "21db021c-cb12-4355-de79-e65a70dd60cf"
      },
      "source": [
        "df2 = df.withColumn('absur', my_absurd_udf('id', 'race').cast(types.FloatType()))\n",
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            " |-- absur: float (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBYSVQQUDG3W",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise: \n",
        "\n",
        "Create a 'hitpoints' field in our df. make it 30000 for halflings, 40000 for elves and 70000 for orcs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCW_GLauDG3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b01bfe06-1a2d-4d60-9736-f13d05963a46"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id|    race|\n",
            "+---+--------+\n",
            "|  0|     orc|\n",
            "|  1|     elf|\n",
            "|  2|     elf|\n",
            "|  3|     elf|\n",
            "|  4|halfling|\n",
            "+---+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elu6edH68ps7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b859b773-98fa-45e8-f2de-3d657238eb33"
      },
      "source": [
        "def hp(race):\n",
        "  hps = {'elf' : 40000, 'halfling' : 30000, 'orc' : 70000}\n",
        "  return hps[race]\n",
        "\n",
        "hp('elf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isK3LdzW9CVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "651ebc5e-cfab-453c-899b-7e1f48c8b23d"
      },
      "source": [
        "hp_udf = functions.udf(hp, returnType=types.IntegerType())\n",
        "\n",
        "df3 = df.withColumn('hp', hp_udf('race'))\n",
        "df3.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+\n",
            "| id|    race|   hp|\n",
            "+---+--------+-----+\n",
            "|  0|     orc|70000|\n",
            "|  1|     elf|40000|\n",
            "|  2|     elf|40000|\n",
            "|  3|     elf|40000|\n",
            "|  4|halfling|30000|\n",
            "+---+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph7NR5HBPuK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cf7c63e-549e-4ca0-8763-dba499e8cde2"
      },
      "source": [
        "print(df3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei4S8ZlJPxXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uS1DeycCKDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "beb2d250-7ec2-40cf-fb2a-d3d3620f7ef0"
      },
      "source": [
        "df3.withColumn('x', functions.when(df3['race'] == 'orc', 70000)\n",
        "                             .when(df3['race'] == 'elf', 40000)\n",
        "                             .otherwise(30000)).show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+-----+\n",
            "| id|    race|   hp|    x|\n",
            "+---+--------+-----+-----+\n",
            "|  0|     orc|70000|70000|\n",
            "|  1|     elf|40000|40000|\n",
            "|  2|     elf|40000|40000|\n",
            "|  3|     elf|40000|40000|\n",
            "|  4|halfling|30000|30000|\n",
            "|  5|halfling|30000|30000|\n",
            "|  6|halfling|30000|30000|\n",
            "|  7|     elf|40000|40000|\n",
            "|  8|     orc|70000|70000|\n",
            "|  9|     elf|40000|40000|\n",
            "| 10|     elf|40000|40000|\n",
            "| 11|     orc|70000|70000|\n",
            "| 12|     elf|40000|40000|\n",
            "| 13|     elf|40000|40000|\n",
            "| 14|     orc|70000|70000|\n",
            "| 15|     orc|70000|70000|\n",
            "| 16|     elf|40000|40000|\n",
            "| 17|     orc|70000|70000|\n",
            "| 18|halfling|30000|30000|\n",
            "| 19|     elf|40000|40000|\n",
            "+---+--------+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgQyvaVLDG4C",
        "colab_type": "text"
      },
      "source": [
        "### Summary statistics\n",
        "\n",
        "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcipDQ8bDG4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56353965-afb2-40db-b426-4d808cb09c7e"
      },
      "source": [
        "df3.stat.corr('id', 'hp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16198084347928346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ku8AlYO96Cm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "319362c1-bb2b-4d63-80ed-bb17bd58c4e5"
      },
      "source": [
        "df3.stat.cov('id', 'hp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15263.157894736842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ug_AfODG4W",
        "colab_type": "text"
      },
      "source": [
        "### .crosstab()\n",
        "\n",
        "Crosstab returns the contingency table for two columns, as a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nen9cfu0DG4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3db1294e-0cfe-499d-f70a-4aa60d7fb0e0"
      },
      "source": [
        "land = functions.udf(lambda : random.choice(['Gondor', 'Shire', 'Mordor']))\n",
        "\n",
        "df4 = df3.withColumn('land', land())\n",
        "df4.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+------+\n",
            "| id|    race|   hp|  land|\n",
            "+---+--------+-----+------+\n",
            "|  0|     orc|70000|Mordor|\n",
            "|  1|     elf|40000| Shire|\n",
            "|  2|     elf|40000|Gondor|\n",
            "|  3|     elf|40000|Mordor|\n",
            "|  4|halfling|30000|Mordor|\n",
            "+---+--------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lYcX3hP-l92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "0dabc3e7-734e-4936-ad86-45f85c2cf7bd"
      },
      "source": [
        "df4.crosstab('race', 'land').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------+------+-----+\n",
            "|race_land|Gondor|Mordor|Shire|\n",
            "+---------+------+------+-----+\n",
            "|      orc|     3|     2|    1|\n",
            "| halfling|     2|     2|    0|\n",
            "|      elf|     3|     4|    3|\n",
            "+---------+------+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_J8ksUfDG4p",
        "colab_type": "text"
      },
      "source": [
        "### Grouping\n",
        "\n",
        "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FPP4b54DG4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "4037a74e-a6bb-4e91-b413-09e5e3573fe3"
      },
      "source": [
        "gd = df4.groupBy('land')\n",
        "gd.sum('hp').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+\n",
            "|  land|sum(hp)|\n",
            "+------+-------+\n",
            "| Shire| 470000|\n",
            "|Mordor| 290000|\n",
            "|Gondor| 180000|\n",
            "+------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qmuADZ9DG45",
        "colab_type": "text"
      },
      "source": [
        "GroupedData has several aggregation functions defined:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8sfeNPzDG48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8wZYh01DG5J",
        "colab_type": "text"
      },
      "source": [
        "We can do several aggregations in a single step, with a number of different syntaxes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUOIYau3DG5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "588102fc-e2e3-489a-ce70-249524b51111"
      },
      "source": [
        "gd.agg(functions.avg('hp'), functions.sum('hp'), functions.max('id')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+-------+-------+\n",
            "|  land|           avg(hp)|sum(hp)|max(id)|\n",
            "+------+------------------+-------+-------+\n",
            "| Shire|           47000.0| 470000|     19|\n",
            "|Mordor|48333.333333333336| 290000|     14|\n",
            "|Gondor|           45000.0| 180000|     18|\n",
            "+------+------------------+-------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWa0ZRyFFFVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "8e5f94cf-a359-40ed-ee3b-2035d9d84538"
      },
      "source": [
        "df4.groupBy(df4['id'] < 10).sum('hp').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-------+\n",
            "|(id < 10)|sum(hp)|\n",
            "+---------+-------+\n",
            "|     true| 430000|\n",
            "|    false| 510000|\n",
            "+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGsYfW6bFprb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "bb0fb8cb-3607-4a64-a500-b78e1c3c0f96"
      },
      "source": [
        "df4.groupBy('land', 'race').sum('hp').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+-------+\n",
            "|  land|    race|sum(hp)|\n",
            "+------+--------+-------+\n",
            "| Shire|     elf| 200000|\n",
            "|Mordor|halfling|  30000|\n",
            "|Mordor|     elf| 120000|\n",
            "|Gondor|     elf|  80000|\n",
            "| Shire|     orc| 210000|\n",
            "| Shire|halfling|  60000|\n",
            "|Mordor|     orc| 140000|\n",
            "|Gondor|     orc|  70000|\n",
            "|Gondor|halfling|  30000|\n",
            "+------+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R98mG-bYDG5X",
        "colab_type": "text"
      },
      "source": [
        "### Intersections\n",
        "\n",
        "Ver much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t0Ljc3EDG5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "10809196-5122-474a-d59d-f94d032ac95a"
      },
      "source": [
        "data = zip(random.choices(['Mordor', 'Gondor', 'Numenor'], k=7), random.choices(range(20000, 50000), k =7))\n",
        "\n",
        "other = spark.createDataFrame(data, schema = ['land', 'gold'])\n",
        "other.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|   land| gold|\n",
            "+-------+-----+\n",
            "| Mordor|33765|\n",
            "|Numenor|27946|\n",
            "|Numenor|27398|\n",
            "| Mordor|36841|\n",
            "| Gondor|27882|\n",
            "| Gondor|37537|\n",
            "|Numenor|46934|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWV8crsaG9Hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7eaccae8-970d-4e32-e8dd-a6426d8f8887"
      },
      "source": [
        "df4.join(other).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1019.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject [id#11L, race#12, hp(race#12) AS hp#293, <lambda>() AS land#352]\n+- LogicalRDD [id#11L, race#12], false\nand\nLogicalRDD [land#716, gold#717L], false\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1295)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1274)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-99f909e0a3fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nProject [id#11L, race#12, hp(race#12) AS hp#293, <lambda>() AS land#352]\\n+- LogicalRDD [id#11L, race#12], false\\nand\\nLogicalRDD [land#716, gold#717L], false\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQZJwRIdDG5l",
        "colab_type": "text"
      },
      "source": [
        "Spark refuses to do cross joins by default. To perform them, we can \n",
        "\n",
        "a) Allow then explicitly:\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "b) Specify the join criterion\n",
        "\n",
        "```python\n",
        "df4.join(new_df, on='id').show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "Q7by-lvfDG5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "bf8a150a-485f-49ac-ec82-6cbe3bca0aed"
      },
      "source": [
        "df4.join(other, on='land').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+--------+-----+-----+\n",
            "|  land| id|    race|   hp| gold|\n",
            "+------+---+--------+-----+-----+\n",
            "|Mordor|  0|     orc|70000|33765|\n",
            "|Mordor|  0|     orc|70000|36841|\n",
            "|Mordor|  3|     elf|40000|33765|\n",
            "|Mordor|  3|     elf|40000|36841|\n",
            "|Mordor|  4|halfling|30000|33765|\n",
            "|Mordor|  4|halfling|30000|36841|\n",
            "|Mordor| 10|     elf|40000|33765|\n",
            "|Mordor| 10|     elf|40000|36841|\n",
            "|Mordor| 13|     elf|40000|33765|\n",
            "|Mordor| 13|     elf|40000|36841|\n",
            "|Mordor| 14|     orc|70000|33765|\n",
            "|Mordor| 14|     orc|70000|36841|\n",
            "|Gondor|  2|     elf|40000|37537|\n",
            "|Gondor|  8|     orc|70000|37537|\n",
            "|Gondor| 12|     elf|40000|37537|\n",
            "|Gondor| 18|halfling|30000|37537|\n",
            "+------+---+--------+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpoE4XczIHhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d527d2b8-a705-464b-b5e3-17d69b9f5953"
      },
      "source": [
        "df4.join(other, on='land', how='left').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+--------+-----+-----+\n",
            "|  land| id|    race|   hp| gold|\n",
            "+------+---+--------+-----+-----+\n",
            "| Shire|  1|     elf|40000| null|\n",
            "| Shire|  5|halfling|30000| null|\n",
            "| Shire|  6|halfling|30000| null|\n",
            "| Shire|  7|     elf|40000| null|\n",
            "| Shire|  9|     elf|40000| null|\n",
            "| Shire| 11|     orc|70000| null|\n",
            "| Shire| 15|     orc|70000| null|\n",
            "| Shire| 16|     elf|40000| null|\n",
            "| Shire| 17|     orc|70000| null|\n",
            "| Shire| 19|     elf|40000| null|\n",
            "|Mordor|  0|     orc|70000|33765|\n",
            "|Mordor|  3|     elf|40000|33765|\n",
            "|Mordor|  4|halfling|30000|33765|\n",
            "|Mordor| 10|     elf|40000|33765|\n",
            "|Mordor| 13|     elf|40000|33765|\n",
            "|Mordor| 14|     orc|70000|33765|\n",
            "|Gondor|  2|     elf|40000| null|\n",
            "|Gondor|  8|     orc|70000| null|\n",
            "|Gondor| 12|     elf|40000| null|\n",
            "|Gondor| 18|halfling|30000| null|\n",
            "+------+---+--------+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XxmUIAlIbXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "85494afb-ff24-4209-ee2d-ef79fd7cf910"
      },
      "source": [
        "df4.join(other, on='land', how='outer').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+--------+-----+-----+\n",
            "|   land|  id|    race|   hp| gold|\n",
            "+-------+----+--------+-----+-----+\n",
            "|Numenor|null|    null| null|27946|\n",
            "|Numenor|null|    null| null|27398|\n",
            "|Numenor|null|    null| null|46934|\n",
            "|  Shire|   1|     elf|40000| null|\n",
            "|  Shire|   5|halfling|30000| null|\n",
            "|  Shire|   6|halfling|30000| null|\n",
            "|  Shire|   7|     elf|40000| null|\n",
            "|  Shire|   9|     elf|40000| null|\n",
            "|  Shire|  12|     elf|40000| null|\n",
            "|  Shire|  13|     elf|40000| null|\n",
            "|  Shire|  14|     orc|70000| null|\n",
            "|  Shire|  16|     elf|40000| null|\n",
            "|  Shire|  17|     orc|70000| null|\n",
            "|  Shire|  19|     elf|40000| null|\n",
            "| Mordor|   0|     orc|70000|33765|\n",
            "| Mordor|   0|     orc|70000|36841|\n",
            "| Mordor|   3|     elf|40000|33765|\n",
            "| Mordor|   3|     elf|40000|36841|\n",
            "| Mordor|   4|halfling|30000|33765|\n",
            "| Mordor|   4|halfling|30000|36841|\n",
            "+-------+----+--------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIbizsHvJgWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3e24650a-3422-4563-8053-47d3e3fc5879"
      },
      "source": [
        "df4.join(other, on = df4['land'] == other['land'], how='outer').show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------+-----+-----+-------+-----+\n",
            "|  id|    race|   hp| land|   land| gold|\n",
            "+----+--------+-----+-----+-------+-----+\n",
            "|null|    null| null| null|Numenor|27946|\n",
            "|null|    null| null| null|Numenor|27398|\n",
            "|null|    null| null| null|Numenor|46934|\n",
            "|   1|     elf|40000|Shire|   null| null|\n",
            "|   5|halfling|30000|Shire|   null| null|\n",
            "+----+--------+-----+-----+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeVbvQ7KQ2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ef3e2021-22b9-4975-bc69-eb357bb80fe2"
      },
      "source": [
        "df4.join(other, on = df4['hp'] < other['gold'], how='outer').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+------+-------+-----+\n",
            "| id|    race|   hp|  land|   land| gold|\n",
            "+---+--------+-----+------+-------+-----+\n",
            "|  0|     orc|70000| Shire|   null| null|\n",
            "|  1|     elf|40000|Gondor|Numenor|46934|\n",
            "|  2|     elf|40000|Gondor|Numenor|46934|\n",
            "|  3|     elf|40000|Mordor|Numenor|46934|\n",
            "|  4|halfling|30000|Mordor| Mordor|33765|\n",
            "|  4|halfling|30000|Mordor| Mordor|36841|\n",
            "|  4|halfling|30000|Mordor| Gondor|37537|\n",
            "|  4|halfling|30000|Mordor|Numenor|46934|\n",
            "|  5|halfling|30000|Gondor| Mordor|33765|\n",
            "|  5|halfling|30000|Gondor| Mordor|36841|\n",
            "|  5|halfling|30000|Gondor| Gondor|37537|\n",
            "|  5|halfling|30000|Gondor|Numenor|46934|\n",
            "|  6|halfling|30000|Mordor| Mordor|33765|\n",
            "|  6|halfling|30000|Mordor| Mordor|36841|\n",
            "|  6|halfling|30000|Mordor| Gondor|37537|\n",
            "|  6|halfling|30000|Mordor|Numenor|46934|\n",
            "|  7|     elf|40000|Gondor|Numenor|46934|\n",
            "|  8|     orc|70000|Gondor|   null| null|\n",
            "|  9|     elf|40000| Shire|Numenor|46934|\n",
            "| 10|     elf|40000| Shire|Numenor|46934|\n",
            "+---+--------+-----+------+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "XpPG9c2wDG5y",
        "colab_type": "text"
      },
      "source": [
        "#### Digression\n",
        "\n",
        "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
        "\n",
        "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed.\n",
        "\n",
        "In Google Colab, we need to do additional work to expose it. We need a Spark session with a web UI port other than 4040. In this example we are using 4050.\n",
        "\n",
        "\n",
        "```python\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -qq ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "```        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBeip8ziDG57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6LhqmJ2DG6I",
        "colab_type": "text"
      },
      "source": [
        "We can erase it with `unpersist`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdoyS7cKDG6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnyp2pQDG6c",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each creature's hitpoints for their location\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oULOu6jmDG6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b5f392b-0bca-4d28-c56b-fcda526882ce"
      },
      "source": [
        "df4.select('*',\n",
        "           df4['hp'] * 2,\n",
        "           (df4['id'] / 4).alias('uncuarto'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string, hp: int, land: string, (hp * 2): int, uncuarto: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-tCefztQbNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75csIlurDG6z",
        "colab_type": "text"
      },
      "source": [
        "1) Calculate the mean and std of hitpoints for each location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM0LueibDG62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "a65842b4-d838-4b96-aec7-8c15ecfe95f5"
      },
      "source": [
        "stats = df4.groupby('land').agg(functions.avg('hp').alias('avg_hp'), \n",
        "                                functions.stddev('hp').alias('std_hp'))\n",
        "stats.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+------------------+\n",
            "|  land|            avg_hp|            std_hp|\n",
            "+------+------------------+------------------+\n",
            "| Shire|57142.857142857145|16035.674514745464|\n",
            "|Mordor|41666.666666666664|14719.601443879743|\n",
            "|Gondor| 41428.57142857143|13451.854182690986|\n",
            "+------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMmvA1lDG7A",
        "colab_type": "text"
      },
      "source": [
        "2) Annotate each creature with the stats corresponding to their location\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_K-op6ODG7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "119d0928-ab19-4e00-af20-1a6a05f47176"
      },
      "source": [
        "annotated = df4.join(stats, on='land')\n",
        "annotated.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+--------+-----+-------+-------+\n",
            "| land| id|    race|   hp| avg_hp| std_hp|\n",
            "+-----+---+--------+-----+-------+-------+\n",
            "|Shire|  1|     elf|40000|47500.0|15000.0|\n",
            "|Shire|  6|halfling|30000|47500.0|15000.0|\n",
            "|Shire|  9|     elf|40000|47500.0|15000.0|\n",
            "|Shire| 11|     orc|70000|47500.0|15000.0|\n",
            "|Shire| 16|     elf|40000|47500.0|15000.0|\n",
            "+-----+---+--------+-----+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ZyrPfBDG7R",
        "colab_type": "text"
      },
      "source": [
        "3) Calculate the z-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbVBaSgXDG7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "eb7c07a4-13a6-4ddf-9057-326803dd94c5"
      },
      "source": [
        "def zscore(xi, mean, std):\n",
        "  return (xi - mean) / std \n",
        "\n",
        "z_udf = functions.udf(zscore, returnType=types.DoubleType())\n",
        "\n",
        "annotated.withColumn('z', z_udf('hp', 'avg_hp', 'std_hp')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+--------+-----+------------------+------------------+--------------------+\n",
            "|  land| id|    race|   hp|            avg_hp|            std_hp|                   z|\n",
            "+------+---+--------+-----+------------------+------------------+--------------------+\n",
            "| Shire|  0|     orc|70000|           47000.0| 16363.91694484477|  1.4055314554285756|\n",
            "| Shire|  2|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
            "| Shire| 10|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
            "| Shire| 12|     elf|40000|           47000.0| 16363.91694484477|  -0.427770442956523|\n",
            "|Mordor|  1|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
            "|Mordor|  6|halfling|30000|48333.333333333336|17224.014243685084| -1.0644053746097526|\n",
            "|Mordor|  8|     orc|70000|48333.333333333336|17224.014243685084|  1.2579336245387982|\n",
            "|Mordor|  9|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
            "|Mordor| 11|     orc|70000|48333.333333333336|17224.014243685084|  1.2579336245387982|\n",
            "|Mordor| 16|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
            "|Mordor| 18|halfling|30000|48333.333333333336|17224.014243685084| -1.0644053746097526|\n",
            "|Mordor| 19|     elf|40000|48333.333333333336|17224.014243685084| -0.4838206248226149|\n",
            "|Gondor|  3|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
            "|Gondor|  4|halfling|30000|           45000.0|17320.508075688773| -0.8660254037844387|\n",
            "|Gondor|  5|halfling|30000|           45000.0|17320.508075688773| -0.8660254037844387|\n",
            "|Gondor|  7|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
            "|Gondor| 13|     elf|40000|           45000.0|17320.508075688773|-0.28867513459481287|\n",
            "|Gondor| 14|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
            "|Gondor| 15|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
            "|Gondor| 17|     orc|70000|           45000.0|17320.508075688773|  1.4433756729740643|\n",
            "+------+---+--------+-----+------------------+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmVFG5xsVW08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47f92335-2593-4c41-c892-25535d3e258e"
      },
      "source": [
        "annotated.select('id', \n",
        "                 'race',\n",
        "                 'land',\n",
        "                 'hp',\n",
        "                 z_udf('hp', 'avg_hp', 'std_hp').alias('z'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string, land: string, hp: int, z: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmqyeVMKVpds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8d6dc03-9471-41f7-c28a-2c2c967d9c1a"
      },
      "source": [
        "annotated.select('id', \n",
        "                 'race',\n",
        "                 'land',\n",
        "                 'hp',\n",
        "                 ((annotated['hp'] - annotated['avg_hp']) / annotated['std_hp']).alias('z'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, race: string, land: string, hp: int, z: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3snR7Y5hDG7k",
        "colab_type": "text"
      },
      "source": [
        "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVGSJ_ZDG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn8fSPdZDG7t",
        "colab_type": "text"
      },
      "source": [
        "### Handling null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlwoNIBiDG7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "bb11adb9-5d15-43ba-9cf3-933a834b942f"
      },
      "source": [
        "with_nulls = df4.join(other, on='land', how='outer')\n",
        "with_nulls.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+--------+-----+-----+\n",
            "|   land|  id|    race|   hp| gold|\n",
            "+-------+----+--------+-----+-----+\n",
            "|Numenor|null|    null| null|27946|\n",
            "|Numenor|null|    null| null|27398|\n",
            "|Numenor|null|    null| null|46934|\n",
            "|  Shire|   1|     elf|40000| null|\n",
            "|  Shire|   5|halfling|30000| null|\n",
            "|  Shire|   6|halfling|30000| null|\n",
            "|  Shire|   7|     elf|40000| null|\n",
            "|  Shire|   9|     elf|40000| null|\n",
            "|  Shire|  11|     orc|70000| null|\n",
            "|  Shire|  15|     orc|70000| null|\n",
            "|  Shire|  16|     elf|40000| null|\n",
            "|  Shire|  17|     orc|70000| null|\n",
            "|  Shire|  19|     elf|40000| null|\n",
            "| Mordor|   0|     orc|70000|33765|\n",
            "| Mordor|   0|     orc|70000|36841|\n",
            "| Mordor|   3|     elf|40000|33765|\n",
            "| Mordor|   3|     elf|40000|36841|\n",
            "| Mordor|   4|halfling|30000|33765|\n",
            "| Mordor|   4|halfling|30000|36841|\n",
            "| Mordor|  10|     elf|40000|33765|\n",
            "+-------+----+--------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QlHmFraaPiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "06fa96a2-970b-42fd-e7e6-69f83e23d07f"
      },
      "source": [
        "with_nulls.dropna().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+--------+-----+-----+\n",
            "|  land| id|    race|   hp| gold|\n",
            "+------+---+--------+-----+-----+\n",
            "|Mordor|  0|     orc|70000|33765|\n",
            "|Mordor|  0|     orc|70000|36841|\n",
            "|Mordor|  3|     elf|40000|33765|\n",
            "|Mordor|  3|     elf|40000|36841|\n",
            "|Mordor|  4|halfling|30000|33765|\n",
            "|Mordor|  4|halfling|30000|36841|\n",
            "|Mordor| 10|     elf|40000|33765|\n",
            "|Mordor| 10|     elf|40000|36841|\n",
            "|Mordor| 13|     elf|40000|33765|\n",
            "|Mordor| 13|     elf|40000|36841|\n",
            "|Mordor| 14|     orc|70000|33765|\n",
            "|Mordor| 14|     orc|70000|36841|\n",
            "|Gondor|  2|     elf|40000|27882|\n",
            "|Gondor|  2|     elf|40000|37537|\n",
            "|Gondor|  8|     orc|70000|27882|\n",
            "|Gondor|  8|     orc|70000|37537|\n",
            "|Gondor| 12|     elf|40000|27882|\n",
            "|Gondor| 12|     elf|40000|37537|\n",
            "|Gondor| 18|halfling|30000|27882|\n",
            "|Gondor| 18|halfling|30000|37537|\n",
            "+------+---+--------+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54IH1cr7aiqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6d62b829-6d0c-48fe-caad-e526a42832b0"
      },
      "source": [
        "with_nulls.dropna(subset='race').show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+----+-----+----+\n",
            "|  land| id|race|   hp|gold|\n",
            "+------+---+----+-----+----+\n",
            "| Shire|  0| orc|70000|null|\n",
            "| Shire|  2| elf|40000|null|\n",
            "| Shire| 10| elf|40000|null|\n",
            "| Shire| 12| elf|40000|null|\n",
            "|Mordor|  1| elf|40000|null|\n",
            "+------+---+----+-----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBBURgd_appj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "ed617501-36eb-4129-ed28-d792e6984401"
      },
      "source": [
        "with_nulls.dropna(how='all', subset=['race', 'land']).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+--------+-----+-----+\n",
            "|   land|  id|    race|   hp| gold|\n",
            "+-------+----+--------+-----+-----+\n",
            "|Numenor|null|    null| null|27946|\n",
            "|Numenor|null|    null| null|27398|\n",
            "|Numenor|null|    null| null|46934|\n",
            "|  Shire|   1|     elf|40000| null|\n",
            "|  Shire|   5|halfling|30000| null|\n",
            "+-------+----+--------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmmzCaYnDG75",
        "colab_type": "text"
      },
      "source": [
        "## SQL querying\n",
        "\n",
        "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT0PkV6XDG79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a56e8fe4-3e6c-4628-f2a2-1e10e9507eba"
      },
      "source": [
        "spark.sql('''SELECT id,\n",
        "                    ((hp - avg_hp) / std_hp) AS z\n",
        "             FROM annotated''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 27))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: annotated; line 3 pos 18\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'annotated' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 53 more\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-6bc392cb2bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spark.sql('''SELECT id,\n\u001b[1;32m      2\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_hp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_hp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m              FROM annotated''')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: annotated; line 3 pos 18'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XibCAlOb5LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotated.registerTempTable('annotated_table')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LlrxOhDG8L",
        "colab_type": "text"
      },
      "source": [
        "Once registered, we can perform queries as complex as we want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9btRD25DG8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "91b06a76-6157-4b0c-be12-3b807d017537"
      },
      "source": [
        "result = spark.sql('''SELECT id,\n",
        "                             ((hp - avg_hp) / std_hp) AS z\n",
        "                      FROM annotated_table''')\n",
        "result.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+\n",
            "| id|                  z|\n",
            "+---+-------------------+\n",
            "|  1|               -0.5|\n",
            "|  5|-1.1666666666666667|\n",
            "|  6|-1.1666666666666667|\n",
            "|  7|               -0.5|\n",
            "|  9|               -0.5|\n",
            "+---+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH86T4G1DG8X",
        "colab_type": "text"
      },
      "source": [
        "## Interoperation with Pandas\n",
        "\n",
        "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8gKF6avdWal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2c7f066f-b2b6-496f-eb94-be8178f96d89"
      },
      "source": [
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.id', 'local-1595056035824'),\n",
              " ('spark.driver.port', '35545'),\n",
              " ('spark.driver.host', '07272e473bf8'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('someconfigkey', 'someconfigvalue')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUHpNVhuDG8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1acedec4-0e6c-4535-e39b-9631279281bf"
      },
      "source": [
        "pd_df = annotated.toPandas()\n",
        "pd_df[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>land</th>\n",
              "      <th>id</th>\n",
              "      <th>race</th>\n",
              "      <th>hp</th>\n",
              "      <th>avg_hp</th>\n",
              "      <th>std_hp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shire</td>\n",
              "      <td>0</td>\n",
              "      <td>orc</td>\n",
              "      <td>70000</td>\n",
              "      <td>47000.0</td>\n",
              "      <td>16363.916945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shire</td>\n",
              "      <td>2</td>\n",
              "      <td>elf</td>\n",
              "      <td>40000</td>\n",
              "      <td>47000.0</td>\n",
              "      <td>16363.916945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Shire</td>\n",
              "      <td>10</td>\n",
              "      <td>elf</td>\n",
              "      <td>40000</td>\n",
              "      <td>47000.0</td>\n",
              "      <td>16363.916945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    land  id race     hp   avg_hp        std_hp\n",
              "0  Shire   0  orc  70000  47000.0  16363.916945\n",
              "1  Shire   2  elf  40000  47000.0  16363.916945\n",
              "2  Shire  10  elf  40000  47000.0  16363.916945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ImEmCEGdrq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "f3d9461a-f259-4a14-a8ba-7c7774d056b3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.hist(pd_df['hp'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 4.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]),\n",
              " array([30000., 34000., 38000., 42000., 46000., 50000., 54000., 58000.,\n",
              "        62000., 66000., 70000.]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD4CAYAAAAaT9YAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN+UlEQVR4nO3de4zldXnH8ffTHS6CFBaYEApuZzEGg6ZBOqVYGmLARmAJ+Id/rIkNtW02qZeqbWPXkFb6RxOwNzFtNFvkYrGgpdoaSVspamrTunQXVgRWygKrLl3YVavW/iGlPv3j94U9O8zlnPM7O2ee8n4lJ/O7nu+zzzn7mTO/y0xkJpKkmn5s2gVIksZniEtSYYa4JBVmiEtSYYa4JBU2s5qDnXrqqTk3N7eaQ0pSeTt37vxWZs4utm5VQ3xubo4dO3as5pCSVF5EfH2pdR5OkaTCDHFJKswQl6TCDHFJKswQl6TCDHFJKmzFEI+ImyLiQEQ8OLDs5Ii4OyIebV/XH9kyJUmLGeaT+C3ApQuWbQXuycxXAPe0eUnSKlsxxDPzn4DvLFh8FXBrm74VeOOE65IkDWHcOzZPy8z9bfop4LSlNoyILcAWgA0bNow53IvT3Na7pjLu3us2TWVcSaPrfWIzuz8NtOSfB8rMbZk5n5nzs7OL3vovSRrTuCH+dEScDtC+HphcSZKkYY0b4p8Brm7TVwN/O5lyJEmjGOYSw9uBfwXOjoh9EfErwHXAL0TEo8Dr27wkaZWteGIzM9+8xKpLJlyLJGlE3rEpSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUmCEuSYUZ4pJUWK8Qj4j3RMRDEfFgRNweEcdOqjBJ0srGDvGIOAP4dWA+M18NrAM2T6owSdLK+h5OmQFeEhEzwHHAf/QvSZI0rLFDPDOfBP4Q+AawH/heZn5u4XYRsSUidkTEjoMHD45fqSTpBfocTlkPXAVsBH4COD4i3rJwu8zclpnzmTk/Ozs7fqWSpBfoczjl9cATmXkwM/8H+BTwc5MpS5I0jD4h/g3ggog4LiICuATYPZmyJEnD6HNMfDtwJ3Af8NX2XNsmVJckaQgzfXbOzPcD759QLZKkEXnHpiQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmGGuCQVZohLUmG9QjwiToqIOyPiaxGxOyJeO6nCJEkrm+m5/w3A32fmmyLiaOC4CdQkSRrS2CEeEScCFwG/BJCZzwDPTKYsSdIw+hxO2QgcBG6OiPsj4saIOH5CdUmShtDncMoMcB7wzszcHhE3AFuB3xncKCK2AFsANmzY0GM4SepnbutdUxl373Wbjthz9/kkvg/Yl5nb2/yddKF+mMzclpnzmTk/OzvbYzhJ0kJjh3hmPgV8MyLObosuAR6eSFWSpKH0vTrlncDH25UpjwNv7V+SJGlYvUI8M3cB8xOqRZI0Iu/YlKTCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCeod4RKyLiPsj4rOTKEiSNLxJfBJ/F7B7As8jSRpRrxCPiDOBTcCNkylHkjSKmZ77fxB4L3DCUhtExBZgC8CGDRvGHmhu611j79vH3us2TWVcSRrG2J/EI+IK4EBm7lxuu8zclpnzmTk/Ozs77nCSpEX0OZxyIXBlROwF7gAujojbJlKVJGkoY4d4Zr4vM8/MzDlgM/D5zHzLxCqTJK3I68QlqbC+JzYByMwvAl+cxHNJkobnJ3FJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCDHFJKswQl6TCxg7xiHhZRHwhIh6OiIci4l2TLEyStLKZHvs+C/xmZt4XEScAOyPi7sx8eEK1SZJWMPYn8czcn5n3ten/AnYDZ0yqMEnSyiZyTDwi5oDXANsXWbclInZExI6DBw9OYjhJUtM7xCPipcBfA+/OzO8vXJ+Z2zJzPjPnZ2dn+w4nSRrQK8Qj4ii6AP94Zn5qMiVJkobV5+qUAD4K7M7MP55cSZKkYfX5JH4h8IvAxRGxqz0un1BdkqQhjH2JYWb+MxATrEWSNCLv2JSkwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwgxxSSrMEJekwnqFeERcGhGPRMSeiNg6qaIkScMZO8QjYh3wZ8BlwDnAmyPinEkVJklaWZ9P4ucDezLz8cx8BrgDuGoyZUmShjHTY98zgG8OzO8DfnbhRhGxBdjSZn8QEY+MOd6pwLfG3Hdscf2Km0ylriGMXdcQ/+Y+/t/16wizrtGsybri+t51/eRSK/qE+FAycxuwre/zRMSOzJyfQEkTZV2jsa7RWNdoXox19Tmc8iTwsoH5M9sySdIq6RPi/wa8IiI2RsTRwGbgM5MpS5I0jLEPp2TmsxHxDuAfgHXATZn50MQqe6Heh2SOEOsajXWNxrpG86KrKzLzSD23JOkI845NSSrMEJekyjJz1R7AscC9wFeAh4Dfa8s3AtuBPcAngKPb8mPa/J62fm7gud7Xlj8CvGFg+aVt2R5ga8+6bgGeAHa1x7lteQAfamM8AJw38FxXA4+2x9UDy38a+Grb50O0Q1lD1rcOuB/47Fro1zJ1Tb1fwN623y5gR1t2MnB3G+NuYP0aqetauiu6nuvX5eO+Xku9J4ao6yTgTuBrwG7gtWukX4vVtRb6dfbA+LuA7wPvnmbPVjvEA3hpmz6qNfEC4JPA5rb8I8Cvtem3AR9p05uBT7Tpc+gC95j2YjxGFyjr2vRZwNFtm3N61HUL8KZFtr8c+Lu23wXA9oGweLx9Xd+mn3sx723bRtv3shH69hvAX3IoLKfar2Xqmnq/6MLy1AXLPkD7DwxsBa5fI3VdC/zWItuO/Hot9Z4Yoq5bgV9t00fTheda6NdidU29XwvGXQc8RXcjztR6tqqHU7LzgzZ7VHskcDHdd13oXrw3tumr2jxt/SUREW35HZn5w8x8gu471vmM+asAlqlrKVcBH2v7fRk4KSJOB94A3J2Z38nM/6T7jnxpW/fjmfnl7F6ljw38G5cVEWcCm4Ab23ww5X4tVtcKVq1fy4z/XF8W9muadS1X79Cv1wrviSVFxInARcBHATLzmcz8LlPu1zJ1LWVV+rWIS4DHMvPrTLFnq35MPCLWRcQu4ABd4Y8B383MZ9sm++hu6YeBW/vb+u8Bp7D4Lf9nLLN85Loyc3tb9fsR8UBE/ElEHLOwriHHP6NNj1wX8EHgvcCP2vwprIF+LVLXc6bdrwQ+FxE72698ADgtM/e36aeA09ZIXQDvaP26KSLWj1nXcu+J5WwEDgI3R8T9EXFjRBzP9Pu1VF0w3X4ttBm4vU1PrWerHuKZ+b+ZeS7dHZ7nA69c7RoWs7CuiHg13XG2VwI/Q/djz2+vZk0RcQVwIDN3rua4K1mmrqn2q/n5zDyP7rdrvj0iLhpc2T7dTOO62sXq+jDwcuBcYD/wR6tc0wxwHvDhzHwN8N90hwKeN6V+LVXXtPv1vHaD45XAXy1ct9o9m9rVKe3Hoy/QnbA4KSKeu/Fo8Pb952/tb+tPBL7N0rf89/5VAAN1XZqZ+9uPQT8Ebqb7pnNYXUOO/2SbHrWuC4ErI2Iv3Y+CFwM3MP1+vaCuiLhtDfSLzHyyfT0AfLrV8HT7MZX29cBaqCszn24fHn4E/Dnj9+vbLP2eWM4+YN/AT5130oXntPu1aF1roF+DLgPuy8yn2/z0epYjHszv8wBmgZPa9EuALwFX0H03GzzJ8LY2/XYOP1H3yTb9Kg4/kfE43UmGmTa9kUMnMl7Vo67T27KgO3xwXZvfxOEnK+7NQycrnqA7UbG+TZ+ci5+suHzE3r2OQycQp9qvZeqaar+A44ETBqb/he7qhD/g8JNOH1gjdZ0+sM176I7rjvV6LfWeGKK2LwFnt+lrW6+m2q9l6pp6vwbGvwN468D81Hq22iH+U3SXpD0APAj8blt+Vit8T2vuMW35sW1+T1t/1sBzXUN3PP0RBs7e0p0N/ve27pqedX2e7lKfB4HbOHQFS9D9QYzH2vr5gef65VbvngUv8nx7nseAP2WESwzb/q/jUFhOtV/L1DXVfrW+fIVDl4pe05afAtxDdynXPw78Z5l2XX/Rxn2A7vcODYbUSK/XUu+JIWo7F9jRavgbukCZar+WqWvq/Wr7Hk/3af7EgWVT65m33UtSYd6xKUmFGeKSVJghLkmFGeKSVJghLkmFGeKSVJghLkmF/R9OZyIEG5Kq6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "copDljPYd7op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beac9f32-17c5-4408-a107-461d664a7534"
      },
      "source": [
        "spark.createDataFrame(pd_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[land: string, id: bigint, race: string, hp: bigint, avg_hp: double, std_hp: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je_NF-zVDG80",
        "colab_type": "text"
      },
      "source": [
        "## Writing out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1YnS75fhR8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39zOmZO9DG82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotated.write.csv('annotated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqYH4Z2xgYLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1d74f44e-da61-4b32-f396-6c37c181b659"
      },
      "source": [
        "!head annotated.csv/part-00033-51449480-3d2e-49a4-8064-f9ed1b86d339-c000.csv  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shire,0,orc,70000,47000.0,16363.91694484477\n",
            "Shire,2,elf,40000,47000.0,16363.91694484477\n",
            "Shire,10,elf,40000,47000.0,16363.91694484477\n",
            "Shire,12,elf,40000,47000.0,16363.91694484477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjEk-9RBDG9C",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
        "\n",
        "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
        "\n",
        "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
        "\n",
        "1. Total ticket amounts per origin\n",
        "2. Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hmTZQVIDG9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "89137b4c-ddbd-4ef9-9eea-f71d7e0c810a"
      },
      "source": [
        "!zcat coupon150720.csv.gz | head -n 5 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\n",
            "79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0\n",
            "79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0\n",
            "79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0\n",
            "79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkk7yA1uDG9a",
        "colab_type": "text"
      },
      "source": [
        "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
        "\n",
        "Remember, you want to calculate:\n",
        "\n",
        "Total ticket amounts per origin\n",
        "\n",
        "Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpAgVqIcDG9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "335d2ffc-6076-431f-db73-72a9fddfcd0e"
      },
      "source": [
        "coupons = spark.sql('''SELECT CAST(_c0 AS BIGINT) AS tkt_number,\n",
        "                              CAST(_c1 AS INT) AS cpn_number,\n",
        "                              _c2 AS origin,\n",
        "                              _c3 AS dest,\n",
        "                              _c4 AS carrier,\n",
        "                              CAST(_c6 AS FLOAT) AS amount\n",
        "                       FROM csv.`coupon150720.csv.gz`''')\n",
        "\n",
        "coupons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[tkt_number: bigint, cpn_number: int, origin: string, dest: string, carrier: string, amount: float]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3geIOiRMDG9n",
        "colab_type": "text"
      },
      "source": [
        "2) Total ticket amounts per origin\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REAxdFlXDG9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "109e8d35-d025-42af-bc0a-a5f399c21fa8"
      },
      "source": [
        "coupons.where(coupons['dest'] == 'MAD')\\\n",
        "       .groupby('origin')\\\n",
        "       .sum('amount').show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|origin|       sum(amount)|\n",
            "+------+------------------+\n",
            "|   PMI| 40547.17005729675|\n",
            "|   YUL|284.44000244140625|\n",
            "|   HEL| 8195.760055541992|\n",
            "|   SXB| 264.4599914550781|\n",
            "|   UIO| 8547.599964141846|\n",
            "+------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzISd6WvDG9u",
        "colab_type": "text"
      },
      "source": [
        "3) Top 10 Airlines by average amount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnCQ4pALDG9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9479dece-1b6d-4c49-9f23-6aaeaec83352"
      },
      "source": [
        "coupons.where(coupons['dest'] == 'MAD')\\\n",
        "       .groupBy('carrier')\\\n",
        "       .avg('amount')\\\n",
        "       .sort('avg(amount)', ascending=False)\\\n",
        "       .show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|carrier|       avg(amount)|\n",
            "+-------+------------------+\n",
            "|     V0| 5418.098665364583|\n",
            "|     AC|  740.619985961914|\n",
            "|     KE| 688.5261500431941|\n",
            "|     SV|  553.174259916265|\n",
            "|     OB| 535.5044420030382|\n",
            "|     AR| 513.5304808843704|\n",
            "|     AV| 450.1950941518613|\n",
            "|     AM| 440.7342111687911|\n",
            "|     C2| 397.8699951171875|\n",
            "|     LA|379.95370341954606|\n",
            "+-------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCyX_XkPDG97",
        "colab_type": "text"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
        "\n",
        "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
        "\n",
        "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
        "\n",
        "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
      ]
    }
  ]
}